{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://raw.githubusercontent.com/PracticumAI/computer_vision/main/images/practicumai_computer_vision.png' alt='Practicum AI: Computer Vision icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Computer Vision Models\n",
    "\n",
    "After a long day at the office, Kevin is getting ready to head home when his boss pops in. \"Kevin, I need you to work on a project for me. We have a dataset of images of livestock, and we need to build a model that can accurately identify the different types of livestock in the images. We also need to be able to segment the livestock from the background. Can you handle this for me?\" Kevin nods, and his boss hands him the dataset. \"I need this done by the end of the week,\" his boss says as he walks away. Kevin looks at the dataset and sighs. It's going to be a long night.\n",
    "\n",
    "As before, the dataset was found on Kaggle. [Check out the dataset information.](https://www.kaggle.com/datasets/amiteshpatra07/cattle-dataset-pig-sheep-cow-horse)\n",
    "\n",
    "<img src=\"images/cattle_segmentation_cover.png\" \n",
    "        alt=\"Image of a cow in a field.\" \n",
    "        width=\"1000\" \n",
    "        height=\"200\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "Kevin already knows that YOLOv8 can handle object detection, but he's not sure how to handle segmentation. He decides to start with YOLOv8 and then look into segmentation. He also knows that he needs to optimize the model to get the best results. He decides to start by looking at the dataset and then move on to building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml \n",
    "from ultralytics import YOLO\n",
    "from ultralytics import settings\n",
    "\n",
    "import helpers_02\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print Pytorch versions and check for GPU\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'  Should be \"True\" if Pytorch was built for GPU: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'  Available GPU: {torch.cuda.get_device_name()}')\n",
    "else:\n",
    "    print('  No GPU available, will use CPU')\n",
    "\n",
    "# Set the number of workers for data loading\n",
    "num_workers = None  # To manually set the number of workers, change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "Like Notebooks 1 and 2.2, we will have to find or download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similar to the helpers_01 functions in the previous notebooks,\n",
    "# The helpers_02.manage_data() function will check for and, if needed,\n",
    "# download the dataset.\n",
    "data_path = helpers_02.manage_data(url=\"https://data.rc.ufl.edu/pub/practicum-ai/Computer_Vision/cattle_segmentation.tar.gz\", \n",
    "                                   filename=\"cattle_segmentation.tar.gz\",\n",
    "                                  folder_name='cattle_segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manage YOLO settings to set correct path to data folder\n",
    "settings.update({\"datasets_dir\": data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the dataset\n",
    "\n",
    "We will take a look at the dataset to see what it contains. We will also look at the annotations file, which contains the bounding box information for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a histogram of the number of images in each class\n",
    "def explore_data(data_path, show_picture=True, show_annotation=True, show_segmentation=True, show_hist=True):\n",
    "\n",
    "    # Define the class names\n",
    "    class_names = ['Cow', 'Horse', 'Pig', 'Sheep', 'Undefined']\n",
    "\n",
    "    # Initialize sample images list to store paths\n",
    "    sample_images = []\n",
    "\n",
    "    # Examine some sample images\n",
    "    if show_picture:\n",
    "        # Get valid image folders \n",
    "        image_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))] \n",
    "\n",
    "        for i in range(5):\n",
    "            folder = random.choice(image_folders) \n",
    "            img_path = os.path.join(data_path, folder, 'images', random.choice(os.listdir(os.path.join(data_path, folder, 'images'))))\n",
    "            sample_images.append(img_path)\n",
    "\n",
    "        # Plot the sample images\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Examine the first annotation file\n",
    "    if show_annotation:\n",
    "        annotation_files = []\n",
    "        for folder in os.listdir(data_path):\n",
    "            if os.path.isdir(os.path.join(data_path, folder)):\n",
    "                annotation_folder = os.path.join(data_path, folder, 'labels')\n",
    "                if os.path.exists(annotation_folder):\n",
    "                    for file in os.listdir(annotation_folder):\n",
    "                        annotation_files.append(os.path.join(annotation_folder, file))\n",
    "        if annotation_files:\n",
    "            annotation_file = annotation_files[0]  # Show only the first annotation file\n",
    "            with open(annotation_file, 'r') as f:\n",
    "                print(f\"File: {annotation_file}\")\n",
    "                for i, line in enumerate(f):\n",
    "                    if i > 4:\n",
    "                        break\n",
    "                    print(f\"  {line.strip()}\")\n",
    "\n",
    "    # Plot the same sample images with their associated, labeled segmentation masks\n",
    "    if show_segmentation and sample_images:\n",
    "        # Find corresponding annotation files for the sample images\n",
    "        sample_annotations = []\n",
    "        for img_path in sample_images:\n",
    "            folder = os.path.dirname(os.path.dirname(img_path))\n",
    "            annotation_path = os.path.join(folder, 'labels', os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "            sample_annotations.append((img_path, annotation_path))\n",
    "\n",
    "        # Plot the sample images with segmentation masks\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, (img_path, annotation_path) in enumerate(sample_annotations):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, *polygon = map(float, line.strip().split())\n",
    "                    polygon = np.array(polygon).reshape(-1, 2) * [img.shape[1], img.shape[0]]\n",
    "                    polygon = polygon.astype(np.int32)\n",
    "                    cv2.fillPoly(mask, [polygon], 255)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].imshow(mask, alpha=0.5, cmap='jet')\n",
    "            axes[i].axis('off')\n",
    "            # Add class name above the segmentation mask\n",
    "            if len(polygon) > 0:\n",
    "                x, y = polygon[0]\n",
    "                axes[i].text(x, y, class_names[int(class_id)], color='white', backgroundcolor='red')\n",
    "        plt.show()\n",
    "\n",
    "    # Make a histogram of the number of images in each class\n",
    "    if show_hist:\n",
    "        def get_class_counts(folder_path):  # Change from data_path to folder_path\n",
    "            class_counts = {}\n",
    "            labels_path = os.path.join(folder_path, 'labels')  # Add labels path\n",
    "            for filename in os.listdir(labels_path):  # Update listdir\n",
    "                with open(os.path.join(labels_path, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        class_id = int(line.split(' ')[0])  # Assuming labels are in YOLO format\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "            return class_counts\n",
    "\n",
    "        train_counts = get_class_counts(os.path.join(data_path, 'train'))  # Add os.path.join\n",
    "        val_counts = get_class_counts(os.path.join(data_path, 'valid'))\n",
    "        test_counts = get_class_counts(os.path.join(data_path, 'test'))\n",
    "        num_classes = len(class_names)\n",
    "\n",
    "        data_counts = {\n",
    "            'train': pd.Series(train_counts),\n",
    "            'val': pd.Series(val_counts),\n",
    "            'test': pd.Series(test_counts)\n",
    "        }\n",
    "        df = pd.DataFrame(data_counts)\n",
    "\n",
    "        df.plot.bar(figsize=(10, 6))\n",
    "        plt.xlabel('Class Name')\n",
    "        plt.xticks(np.arange(num_classes), class_names)\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.title('Distribution of Images per Class')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "explore_data(data_path, show_picture=True, show_annotation=True, show_segmentation=True, show_hist=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the fruit detection notebook, there are some issues with this cattle dataset!\n",
    "- Not all the animals in some of the images are annotated.\n",
    "- The annotation file names are the same as the image names, but with a .txt extension.\n",
    "- The annotations file contains the class ID of the animal (0 corresponds to 'Cow', etc.), and the segmentation coordinates. \n",
    "- The segmentation coordinates are the pixel locations for the vertices of the segmenting polygons.\n",
    "- The segmentation coordinates are normalized, meaning that they are scaled to be between 0 and 1.\n",
    "- The dataset is very imbalanced, with a lot more pigs than other animals, and comparatively almost no horses.\n",
    "\n",
    "## 4. Create the YAML file\n",
    "YAML stands for \"YAML Ain't Markup Language\" and is a human-readable data serialization format. A YAML file is used to define the dataset configuration for training a YOLOv8 model. YAML configuration files are popular in deep learning because they are relatively easy for humans to read and write, with the goal of increasing transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a YAML file for the YOLOv8 model configuration\n",
    "\n",
    "def create_yaml(data_path, class_names, yaml_file='cattle_segmentation_data.yaml'):\n",
    "    # Creates a YOLOv8 data.yaml file.\n",
    "    \n",
    "    yaml_dict = {\n",
    "        # 'path': data_path,  # Path to your dataset\n",
    "        'train': data_path + '/train/images',  # Relative path to training images\n",
    "        'val': data_path + '/valid/images',    # Relative path to validation images\n",
    "        'test': data_path + '/test/images',    # Relative path to testing images\n",
    "\n",
    "        'num_classes': len(class_names),   # Number of classes\n",
    "        'names': class_names      # List of class names\n",
    "    }\n",
    "\n",
    "    with open(yaml_file, 'w') as outfile:\n",
    "        yaml.dump(yaml_dict, outfile, default_flow_style=False)\n",
    "\n",
    "    print(f'YAML file created: {yaml_file}')\n",
    "\n",
    "data_dir = 'cattle_segmentation'\n",
    "class_names = ['Cow', 'Horse', 'Pig', 'Sheep', 'Undefined']\n",
    "\n",
    "create_yaml(data_path, class_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and fit the model\n",
    "\n",
    "We will create a YOLOv8 model and fit it to the data. YOLOv8 has a lot of hyperparameters that can be tuned, but we will use the default values for now. For more information on its hyperparameters, [check out YOLOv8's documentation](https://docs.ultralytics.com/modes/train/).\n",
    "\n",
    "Another neat feature of YOLOv8 is that by default it provides several evaluation metrics, such as the loss, precision, recall, and F1 score. This is very useful for monitoring the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make and train the YOLOv8 model using Lightning wrapper\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING YOLOv8 SEGMENTATION MODEL WITH LIGHTNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train using Lightning wrapper for better experiment tracking and consistency\n",
    "model, trainer, yolo_model = helpers_02.train_yolo_model(\n",
    "    data_config='cattle_segmentation_data.yaml',\n",
    "    model_type='yolov8n-seg',  # Segmentation model\n",
    "    max_epochs=10,\n",
    "    img_size=640,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    experiment_name='cattle_segmentation'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed! Model evaluation will be shown below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the results\n",
    "\n",
    "Let's look at those evaluation metrics we mentioned above. YOLOv8 creates a **runs** folder that stores each training run. We'll pull them up here and examine what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation results\n",
    "\n",
    "# Find the latest training run\n",
    "training = sorted(os.listdir('runs/segment/.'))\n",
    "latest_training = training[-1]\n",
    "print(f'Latest training run: {latest_training}')\n",
    "\n",
    "# Plot the .png files in the latest training run\n",
    "for file in os.listdir(f'runs/segment/{latest_training}'):\n",
    "    if file.endswith('.png'):\n",
    "        # Exclude the normalized confusion matrix since it's redundant\n",
    "        if 'normalized' in file:\n",
    "            continue\n",
    "        img = Image.open(f'runs/segment/{latest_training}/{file}')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Re-Refresher\n",
    "\n",
    "Here is a link to the Stat Term section in Notebook [02_boxes_of_fruit.ipynb], in case you need to look at it again. \n",
    "\n",
    "Link: [Breaking down the graphs](02_boxes_of_fruit.ipynb#Breaking-down-the-graphs)\n",
    "\n",
    "> **Note:** On Google Colab, the link will only open notebook 02_boxes_of_fruit, you'll have to navigate down to the \"Breaking down the graphs\" section on your own!\n",
    "\n",
    "In addition to the evaluation graphs, we can look at the predictions themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the labels and predictions from the last training run\n",
    "\n",
    "# Load the images\n",
    "img1 = Image.open(f'runs/segment/{latest_training}/val_batch2_labels.jpg')\n",
    "img2 = Image.open(f'runs/segment/{latest_training}/val_batch2_pred.jpg')\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions above aren't perfect, but they are pretty good! The model is able to detect the animals in the images and put segmentation masks over them. For the most part, the segmentation masks hold up to the animals' shapes, but it doesn't always predict the right class. For example, the model sometimes predicts a cow as a pig, or a horse as a sheep. What are some ways we could improve the model's performance?\n",
    "\n",
    "## 7. Inference\n",
    "How does the model fare on some test images? After you run the cell below:\n",
    "1. Find your own image of a cow, pig, sheep, or horse (or any other animal you like).\n",
    "2. Upload it to this folder.\n",
    "3. Add or edit the code below to run on the new image rather than images in the test folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get ten random test images\n",
    "test_images = []\n",
    "for img_name in os.listdir(os.path.join(data_path, 'test', 'images')):\n",
    "    img_path = os.path.join(data_path, 'test', 'images', img_name)\n",
    "    test_images.append(img_path)\n",
    "test_images = random.sample(test_images, 10)\n",
    "\n",
    "# Plot the test images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, img_path in enumerate(test_images):\n",
    "    img = Image.open(img_path)\n",
    "    axes[i // 5, i % 5].imshow(img)\n",
    "    axes[i // 5, i % 5].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Use our Lightning wrapper's evaluation function for consistent results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION ON TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "test_metrics = helpers_02.evaluate_yolo_model(yolo_model, 'cattle_segmentation_data.yaml')\n",
    "\n",
    "# Run the model on the test images using our visualization function\n",
    "print(\"\\nVisualization of predictions:\")\n",
    "# Select first 5 images for detailed visualization\n",
    "sample_test_images = test_images[:5]\n",
    "helpers_02.visualize_yolo_results(yolo_model, sample_test_images, conf_threshold=0.25)\n",
    "\n",
    "# Also show individual predictions as before for detailed inspection\n",
    "print(\"\\nDetailed individual predictions:\")\n",
    "infer_results = yolo_model(sample_test_images)\n",
    "\n",
    "for img_path, result in zip(sample_test_images, infer_results):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((640, 640))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Check if segmentation masks are available\n",
    "    if result.masks is not None:\n",
    "        # Access segmentation masks and class information\n",
    "        masks = result.masks.data.cpu().numpy()  # Masks are on the CPU\n",
    "        class_ids = result.boxes.cls.cpu().numpy()  # Class IDs are on the CPU\n",
    "        confidences = result.boxes.conf.cpu().numpy()  # Confidences are on the CPU\n",
    "\n",
    "        for mask, class_id, conf in zip(masks, class_ids, confidences):\n",
    "            # Create a mask overlay\n",
    "            mask = mask.squeeze()\n",
    "            plt.imshow(mask, alpha=0.05, cmap='spring')\n",
    "\n",
    "            # Get the contour of the mask\n",
    "            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for contour in contours:\n",
    "                plt.plot(contour[:, 0, 0], contour[:, 0, 1], color='red', linewidth=2)\n",
    "            \n",
    "            # Display class name and confidence score for each instance\n",
    "            mask_center_x = int(mask.shape[1] / 2)\n",
    "            mask_center_y = int(mask.shape[0] / 2)\n",
    "            plt.text(mask_center_x, mask_center_y, f\"{result.names[int(class_id)]} {conf:.2f}\", color='white', backgroundcolor='red')\n",
    "    else:\n",
    "        plt.text(320, 320, \"No class detected\", fontsize=12, color='white', backgroundcolor='red', ha='center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore hyperparameters\n",
    "\n",
    "Now that you have a good baseline, consider how you might deal with this model's issues.\n",
    "- How would you address issues in the dataset?\n",
    "- How would you optimize training?\n",
    "\n",
    "A good first place to start would be YOLOv8's documentation, so you can understand what hyperparameters you have access to and how changing them will affect training. Make some adjustments and see how high you can get your cattle segmentation F1 score!\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "- You might have noticed the *yolov8n-seg.pt* file that is added to the folder when you load YOLO. That is the pre-trained model using the [COCO dataset](https://cocodataset.org/#home). The YOLOv8 documentation linked above provides instructions for transfer learning and fine-tuning with the pre-trained model. Give it a shot!\n",
    "\n",
    "- The dataset is imbalanced, with a lot more pigs than other animals. How would you address this issue? (Image augmentation is a little more difficult with segmentation tasks, but there are still some techniques you could try. Check out the [albumentations library](https://albumentations.ai/docs/getting_started/image_augmentation/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison and Hyperparameter Tuning\n",
    "\n",
    "Now that we have a Lightning wrapper for YOLOv8, let's explore some advanced features that make model comparison and optimization easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different YOLO model sizes\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING MULTIPLE MODELS FOR COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train a smaller YOLOv8 nano model for comparison\n",
    "print(\"Training YOLOv8 Nano (faster, smaller)...\")\n",
    "nano_model, nano_trainer, nano_yolo = helpers_02.train_yolo_model(\n",
    "    data_config='cattle_segmentation_data.yaml',\n",
    "    model_type='yolov8n-seg',\n",
    "    max_epochs=5,  # Fewer epochs for demonstration\n",
    "    img_size=640,\n",
    "    experiment_name='cattle_nano'\n",
    ")\n",
    "\n",
    "# Train a larger YOLOv8 small model for comparison\n",
    "print(\"\\nTraining YOLOv8 Small (slower, larger)...\")\n",
    "small_model, small_trainer, small_yolo = helpers_02.train_yolo_model(\n",
    "    data_config='cattle_segmentation_data.yaml',\n",
    "    model_type='yolov8s-seg',\n",
    "    max_epochs=5,  # Fewer epochs for demonstration\n",
    "    img_size=640,\n",
    "    experiment_name='cattle_small'\n",
    ")\n",
    "\n",
    "# Compare the models\n",
    "models_to_compare = {\n",
    "    'YOLOv8n-seg (Nano)': nano_yolo,\n",
    "    'YOLOv8s-seg (Small)': small_yolo\n",
    "}\n",
    "\n",
    "comparison_results = helpers_02.compare_yolo_models(\n",
    "    models_to_compare, \n",
    "    'cattle_segmentation_data.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Automated hyperparameter tuning\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOMATED HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparams_grid = {\n",
    "    'learning_rate': [0.01, 0.005],\n",
    "    'weight_decay': [0.0005, 0.001],\n",
    "    'img_size': [640]  # Keep constant for this example\n",
    "}\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "tuning_results = helpers_02.train_yolo_with_hyperparameter_tuning(\n",
    "    data_config='cattle_segmentation_data.yaml',\n",
    "    model_type='yolov8n-seg',\n",
    "    hyperparams_grid=hyperparams_grid,\n",
    "    max_epochs=3  # Short runs for demonstration\n",
    ")\n",
    "\n",
    "print(\"\\nHyperparameter tuning completed!\")\n",
    "print(\"Check the results above to see which configuration performed best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Benefits of Lightning Integration\n",
    "\n",
    "By wrapping YOLOv8 with PyTorch Lightning, we've gained several advantages while maintaining the power of YOLO's optimized detection pipeline:\n",
    "\n",
    "### ðŸš€ **Enhanced Experiment Management**\n",
    "- **Consistent logging**: All experiments are logged to TensorBoard in a standardized format\n",
    "- **Easy comparison**: Models can be compared side-by-side with consistent metrics\n",
    "- **Reproducible results**: Seed setting and configuration tracking ensure reproducibility\n",
    "\n",
    "### ðŸ”§ **Simplified Training Workflow**\n",
    "- **Unified API**: Same function calls as our classification notebooks (helpers_01.py)\n",
    "- **Automatic callbacks**: Early stopping and checkpointing handled automatically\n",
    "- **Device management**: Automatic GPU/CPU detection and multi-GPU support\n",
    "\n",
    "### ðŸ“Š **Better Model Comparison**\n",
    "- **Standardized evaluation**: All models evaluated with the same metrics\n",
    "- **Automated hyperparameter tuning**: Grid search with organized results\n",
    "- **Performance tracking**: Easy identification of best-performing configurations\n",
    "\n",
    "### ðŸ’¡ **Educational Consistency**\n",
    "- **Learning transfer**: Knowledge from classification notebooks applies here\n",
    "- **Code reusability**: Similar patterns and functions across all notebooks\n",
    "- **Best practices**: Industry-standard training and evaluation workflows\n",
    "\n",
    "### ðŸŽ¯ **Maintained YOLO Advantages**\n",
    "- **Optimized performance**: Still uses YOLO's efficient training pipeline\n",
    "- **All YOLO features**: Access to YOLO's data augmentation, loss functions, etc.\n",
    "- **Native compatibility**: Works with existing YOLO datasets and configurations\n",
    "\n",
    "This hybrid approach gives you the best of both worlds: the ease and standardization of Lightning with the specialized efficiency of YOLOv8 for object detection and segmentation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
