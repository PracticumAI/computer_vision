{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data Imbalance in Computer Vision\n",
    "\n",
    "Welcome to the fourth notebook in our Computer Vision series! In the previous notebooks, we've been working with a **subsampled** version of the bee vs wasp dataset that was relatively balanced across classes. However, real-world datasets often suffer from **class imbalance** - where some classes have significantly more examples than others.\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Work with the **full** bee vs wasp dataset (not the subsampled version)\n",
    "2. Explore the class imbalance problem\n",
    "3. Learn techniques to handle imbalanced data using PyTorch Lightning\n",
    "4. Compare model performance with and without imbalance handling techniques\n",
    "\n",
    "## What is Class Imbalance?\n",
    "\n",
    "Class imbalance occurs when the number of samples in different classes varies significantly. For example:\n",
    "- Class A: 1000 samples\n",
    "- Class B: 100 samples  \n",
    "- Class C: 50 samples\n",
    "- Class D: 10 samples\n",
    "\n",
    "This imbalance can cause models to:\n",
    "- **Bias toward majority classes**: The model learns to predict the most common class more often\n",
    "- **Poor performance on minority classes**: Rare classes get insufficient training examples\n",
    "- **Misleading accuracy metrics**: High overall accuracy can hide poor performance on important minority classes\n",
    "\n",
    "## Techniques for Handling Class Imbalance\n",
    "\n",
    "We'll explore several approaches:\n",
    "1. **Weighted Random Sampling**: Oversample minority classes during training\n",
    "2. **Class Weights in Loss Function**: Give higher penalty for misclassifying minority classes\n",
    "3. **Evaluation Metrics**: Use metrics that account for class imbalance (precision, recall, F1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cpu\n",
      "PyTorch Lightning version: 2.5.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our helper functions\n",
    "import helpers_01\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Number of Workers for Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 workers for data loading.\n"
     ]
    }
   ],
   "source": [
    "# Set the number of workers to use for data loading\n",
    "num_workers = None  # To manually set the number of workers, change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Full (Imbalanced) Dataset\n",
    "\n",
    "Now we'll load the **full** bee vs wasp dataset, which contains the natural class imbalance. This is different from the subsampled dataset used in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data at: data\\bee_vs_wasp\n"
     ]
    }
   ],
   "source": [
    "# Get the full dataset (not the subsampled version)\n",
    "full_data_path = helpers_01.manage_full_data(\n",
    "    folder_name=\"bee_vs_wasp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Class Imbalance\n",
    "\n",
    "Let's first load the data **without** any imbalance handling to see the natural distribution of classes.\n",
    "\n",
    "Before we proceed, we'll validate that the dataset is properly structured and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data path exists before proceeding\n",
    "import os\n",
    "\n",
    "# Double-check that we have a valid dataset path\n",
    "if \"full_data_path\" not in locals() or full_data_path is None:\n",
    "    print(\"‚ùå Error: No dataset path found.\")\n",
    "    print(\"Please run the previous cell (Get the full dataset) first.\")\n",
    "    raise NameError(\n",
    "        \"Variable 'full_data_path' not found. Please run the dataset loading cell first.\"\n",
    "    )\n",
    "\n",
    "if not os.path.exists(full_data_path):\n",
    "    print(f\"‚ùå Error: Dataset path does not exist: {full_data_path}\")\n",
    "    print(\"\\nThe dataset folder should contain subdirectories:\")\n",
    "    print(\"- bee/\")\n",
    "    print(\"- wasp/\")\n",
    "    print(\"- other_insect/\")\n",
    "    print(\"- other_noinsect/\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Verify the dataset is properly extracted\")\n",
    "    print(\"2. Check the folder structure\")\n",
    "    print(\"3. Re-run the dataset loading cell if needed\")\n",
    "    raise FileNotFoundError(f\"Dataset path is invalid: {full_data_path}\")\n",
    "\n",
    "# Verify it's a directory with the expected structure\n",
    "expected_classes = [\"bee\", \"wasp\", \"other_insect\", \"other_noinsect\"]\n",
    "missing_classes = []\n",
    "for class_name in expected_classes:\n",
    "    class_path = os.path.join(full_data_path, class_name)\n",
    "    if not os.path.exists(class_path):\n",
    "        missing_classes.append(class_name)\n",
    "\n",
    "if missing_classes:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing expected class directories: {missing_classes}\")\n",
    "    print(f\"Found directories in {full_data_path}:\")\n",
    "    try:\n",
    "        actual_dirs = [\n",
    "            d\n",
    "            for d in os.listdir(full_data_path)\n",
    "            if os.path.isdir(os.path.join(full_data_path, d))\n",
    "        ]\n",
    "        print(f\"  {actual_dirs}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not list directories: {e}\")\n",
    "    print(\"\\nThis may cause issues during data loading.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset validation successful!\")\n",
    "    print(f\"üìÅ Using dataset at: {full_data_path}\")\n",
    "    print(f\"üìä Found all expected class directories: {expected_classes}\")\n",
    "\n",
    "# Load data without weighted sampling to see the natural imbalance\n",
    "data_module_unbalanced = helpers_01.load_imbalanced_data(\n",
    "    full_data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=True,\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    use_weighted_sampler=False,  # No weighted sampling to see natural distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset is highly imbalanced! Some classes have thousands of images while one has only hundreds. This imbalance can significantly impact model performance.\n",
    "\n",
    "## 5. Train a Baseline Model (No Imbalance Handling)\n",
    "\n",
    "Let's first train a model without any imbalance handling techniques to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BASELINE MODEL (No Imbalance Handling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train baseline model without imbalance handling\n",
    "baseline_model, baseline_trainer = helpers_01.train_model(\n",
    "    data_module=data_module_unbalanced,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline Model Results:\")\n",
    "baseline_results = helpers_01.test_model(\n",
    "    data_module_unbalanced, baseline_model, baseline_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model with Weighted Random Sampling\n",
    "\n",
    "Now let's train a model using **weighted random sampling**. This technique oversamples minority classes during training so the model sees a more balanced distribution of examples.\n",
    "\n",
    "Note: We'll load the data again but won't show the class distribution since we already saw it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data WITH weighted sampling to handle imbalance\n",
    "data_module_weighted = helpers_01.load_imbalanced_data(\n",
    "    full_data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=True,\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    use_weighted_sampler=True,  # Enable weighted sampling\n",
    "    show_class_distribution=False,  # Don't show distribution again\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH WEIGHTED SAMPLING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train model with weighted sampling\n",
    "weighted_model, weighted_trainer = helpers_01.train_model(\n",
    "    data_module=data_module_weighted,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nWeighted Sampling Model Results:\")\n",
    "weighted_results = helpers_01.test_model(\n",
    "    data_module_weighted, weighted_model, weighted_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model with Class Weights in Loss Function\n",
    "\n",
    "Another approach is to use **class weights** in the loss function. This gives higher penalty for misclassifying minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with weighted loss function\n",
    "class WeightedLossCNN(helpers_01.SimpleCNN):\n",
    "    \"\"\"CNN model with weighted loss function for handling class imbalance\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=4,\n",
    "        learning_rate=0.001,\n",
    "        input_shape=(3, 80, 80),\n",
    "        dropout_rate=0.3,\n",
    "        conv_padding=1,\n",
    "        class_weights=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_classes, learning_rate, input_shape, dropout_rate, conv_padding\n",
    "        )\n",
    "\n",
    "        # Store class weights for loss function\n",
    "        if class_weights is not None:\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        # Use weighted loss if class weights are provided\n",
    "        if self.class_weights is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Calculate class weights based on inverse frequency\n",
    "class_names, class_counts = data_module_unbalanced.get_class_info()\n",
    "total_samples = sum(class_counts.values())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "class_weights = []\n",
    "for class_name in class_names:\n",
    "    weight = total_samples / (num_classes * class_counts[class_name])\n",
    "    class_weights.append(weight)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(f\"Calculated class weights: {class_weights_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH WEIGHTED LOSS FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create model with weighted loss\n",
    "weighted_loss_model = WeightedLossCNN(\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,\n",
    "    class_weights=class_weights_tensor,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"weighted_loss_experiment\")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, verbose=False, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"weighted-loss-best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "weighted_loss_trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "# Train the model (use unbalanced data module since we're handling imbalance in the loss function)\n",
    "weighted_loss_trainer.fit(weighted_loss_model, datamodule=data_module_unbalanced)\n",
    "\n",
    "print(\"\\nWeighted Loss Model Results:\")\n",
    "weighted_loss_results = helpers_01.test_model(\n",
    "    data_module_unbalanced, weighted_loss_model, weighted_loss_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Evaluation and Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of all three approaches using detailed metrics that are appropriate for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation(model, data_module, trainer, model_name):\n",
    "    \"\"\"Perform detailed evaluation with metrics appropriate for imbalanced datasets\"\"\"\n",
    "\n",
    "    # Get predictions on validation set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    val_loader = data_module.val_dataloader()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Get class names\n",
    "    class_names, _ = data_module.get_class_info()\n",
    "\n",
    "    print(f\"\\n{model_name} - Detailed Evaluation:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds, target_names=class_names, digits=3, zero_division=0\n",
    "    )\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar=True,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        annot_kws={\"size\": 12},\n",
    "    )\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  {class_name}: {per_class_acc[i]:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": all_preds,\n",
    "        \"labels\": all_labels,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"per_class_accuracy\": per_class_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate all three models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_eval = detailed_evaluation(\n",
    "    baseline_model,\n",
    "    data_module_unbalanced,\n",
    "    baseline_trainer,\n",
    "    \"Baseline (No Imbalance Handling)\",\n",
    ")\n",
    "weighted_eval = detailed_evaluation(\n",
    "    weighted_model, data_module_weighted, weighted_trainer, \"Weighted Sampling\"\n",
    ")\n",
    "weighted_loss_eval = detailed_evaluation(\n",
    "    weighted_loss_model,\n",
    "    data_module_unbalanced,\n",
    "    weighted_loss_trainer,\n",
    "    \"Weighted Loss Function\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Comparison\n",
    "\n",
    "Let's create a summary comparison of all three approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison\n",
    "class_names, class_counts = data_module_unbalanced.get_class_info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nPer-Class Accuracy Comparison:\")\n",
    "print(f\"{'Class':<15} {'Baseline':<12} {'Weighted Samp':<14} {'Weighted Loss':<13}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    baseline_acc = baseline_eval[\"per_class_accuracy\"][i]\n",
    "    weighted_acc = weighted_eval[\"per_class_accuracy\"][i]\n",
    "    weighted_loss_acc = weighted_loss_eval[\"per_class_accuracy\"][i]\n",
    "\n",
    "    print(\n",
    "        f\"{class_name:<15} {baseline_acc:<12.3f} {weighted_acc:<14.3f} {weighted_loss_acc:<13.3f}\"\n",
    "    )\n",
    "\n",
    "# Calculate macro and micro averages\n",
    "print(f\"\\n{'Metric':<20} {'Baseline':<12} {'Weighted Samp':<14} {'Weighted Loss':<13}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Macro average (unweighted mean of per-class accuracies)\n",
    "baseline_macro = np.mean(baseline_eval[\"per_class_accuracy\"])\n",
    "weighted_macro = np.mean(weighted_eval[\"per_class_accuracy\"])\n",
    "weighted_loss_macro = np.mean(weighted_loss_eval[\"per_class_accuracy\"])\n",
    "\n",
    "print(\n",
    "    f\"{'Macro Avg Accuracy':<20} {baseline_macro:<12.3f} {weighted_macro:<14.3f} {weighted_loss_macro:<13.3f}\"\n",
    ")\n",
    "\n",
    "# Overall accuracy\n",
    "baseline_overall = np.sum(np.diag(baseline_eval[\"confusion_matrix\"])) / np.sum(\n",
    "    baseline_eval[\"confusion_matrix\"]\n",
    ")\n",
    "weighted_overall = np.sum(np.diag(weighted_eval[\"confusion_matrix\"])) / np.sum(\n",
    "    weighted_eval[\"confusion_matrix\"]\n",
    ")\n",
    "weighted_loss_overall = np.sum(\n",
    "    np.diag(weighted_loss_eval[\"confusion_matrix\"])\n",
    ") / np.sum(weighted_loss_eval[\"confusion_matrix\"])\n",
    "\n",
    "print(\n",
    "    f\"{'Overall Accuracy':<20} {baseline_overall:<12.3f} {weighted_overall:<14.3f} {weighted_loss_overall:<13.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "Based on our experiments with handling class imbalance, here are the key insights:\n",
    "\n",
    "### Techniques Compared:\n",
    "\n",
    "1. **Baseline (No Imbalance Handling)**:\n",
    "   - Uses the natural, imbalanced distribution\n",
    "   - Typically biased toward majority classes\n",
    "   - May achieve high overall accuracy but poor performance on minority classes\n",
    "\n",
    "2. **Weighted Random Sampling**:\n",
    "   - Oversamples minority classes during training\n",
    "   - Ensures the model sees a balanced distribution of examples\n",
    "   - Can improve performance on minority classes\n",
    "\n",
    "3. **Weighted Loss Function**:\n",
    "   - Applies higher penalty for misclassifying minority classes\n",
    "   - Keeps the original data distribution but adjusts learning signal\n",
    "   - Often provides a good balance between majority and minority class performance\n",
    "\n",
    "### When to Use Each Technique:\n",
    "\n",
    "- **Weighted Sampling**: When you have sufficient data and computational resources for oversampling\n",
    "- **Weighted Loss**: When you want to maintain the original data distribution but adjust the learning process\n",
    "- **Combination**: Sometimes combining both techniques can yield the best results\n",
    "\n",
    "### Important Considerations:\n",
    "\n",
    "1. **Evaluation Metrics**: For imbalanced datasets, focus on:\n",
    "   - Per-class accuracy\n",
    "   - Macro-averaged metrics (treats all classes equally)\n",
    "   - Precision, recall, and F1-score for each class\n",
    "\n",
    "2. **Real-world Impact**: Consider the cost of misclassifying different classes. Sometimes it's more important to correctly identify rare but critical cases.\n",
    "\n",
    "3. **Data Quality**: Ensure minority classes have sufficient diversity and quality examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Experiment on Your Own\n",
    "\n",
    "Try modifying the following parameters to see how they affect performance:\n",
    "\n",
    "1. **Class weights**: Experiment with different weighting schemes\n",
    "2. **Sampling strategy**: Try different oversampling ratios\n",
    "3. **Model architecture**: Add more layers or change dropout rates\n",
    "4. **Learning rate**: Different learning rates for imbalanced data\n",
    "5. **Batch size**: Smaller batches might help with minority classes\n",
    "\n",
    "Use the code cells below to run your own experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment cell - modify parameters here\n",
    "# Example: Try different class weights\n",
    "\n",
    "# Calculate different class weights (more aggressive)\n",
    "aggressive_weights = []\n",
    "for class_name in class_names:\n",
    "    weight = (\n",
    "        total_samples / class_counts[class_name]\n",
    "    ) ** 0.5  # Square root for less aggressive weighting\n",
    "    aggressive_weights.append(weight)\n",
    "\n",
    "aggressive_weights_tensor = torch.tensor(aggressive_weights, dtype=torch.float32)\n",
    "print(f\"Aggressive class weights: {aggressive_weights_tensor}\")\n",
    "\n",
    "# You can create and train a new model with these weights here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional experiment cell for your own modifications\n",
    "# Try different batch sizes, learning rates, or architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
