{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71aba844",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf542b",
   "metadata": {},
   "source": [
    "# Data Augmentation for Computer Vision\n",
    "\n",
    "In the [previous notebook](01.1_bees_vs_wasps.ipynb), we explored CNNs, dropout, and padding to improve our bee vs wasp classifier. However, we still faced the challenge of **overfitting** - our model performed well on training data but struggled to generalize to new images.\n",
    "\n",
    "**Data augmentation** is another powerful technique to combat overfitting by artificially expanding our training dataset through transformations that preserve the essential characteristics of our images while adding variety.\n",
    "\n",
    "## What is Data Augmentation?\n",
    "\n",
    "<img src='images/data_augmentation.png'  alt='Graphic of data augmentation, showing rotations, flips, and color changes' align=right>\n",
    "\n",
    "Data augmentation involves applying random transformations to training images to create new, slightly different versions. For example:\n",
    "- **Rotation**: Slightly rotating images\n",
    "- **Flipping**: Horizontal or vertical flips\n",
    "- **Scaling**: Zooming in or out\n",
    "- **Color adjustments**: Changing brightness, contrast, or saturation\n",
    "- **Cropping**: Taking random sections of images\n",
    "\n",
    "These transformations help the model learn to recognize objects regardless of their orientation, lighting conditions, or position in the frame.\n",
    "\n",
    "## Benefits of Data Augmentation\n",
    "\n",
    "1. **Reduces overfitting**: By seeing more variations of the data\n",
    "2. **Improves generalization**: Model becomes robust to variations\n",
    "3. **Increases effective dataset size**: Without collecting new data\n",
    "4. **Better real-world performance**: Handles variations in test conditions\n",
    "\n",
    "Let's explore how to implement and evaluate data augmentation with our bee vs wasp dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f6e77",
   "metadata": {},
   "source": [
    "## 1. Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633adf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Many functions are moved to helpers_01.py to keep this file clean.\n",
    "import helpers_01\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbb231",
   "metadata": {},
   "source": [
    "## 2. Check PyTorch installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeeed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print Pytorch versions and check for GPU\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f'  Should be \"True\" if Pytorch was built for GPU: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Available GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"  No GPU available, will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f06a8",
   "metadata": {},
   "source": [
    "## 3. Load and prepare the data\n",
    "\n",
    "We'll use the same subsampled bee vs wasp dataset from the previous notebook. If you need to download the data, it is [hosted for public download from HiPerGator as a `tar.gz` file](https://data.rc.ufl.edu/pub/practicum-ai/Deep_Learning_Foundations/bee_vs_wasp.tar.gz).\n",
    "\n",
    "As before, we'll set the number of workers for Lightning data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a7438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the data path\n",
    "data_path = helpers_01.manage_data(folder_name=\"bee_vs_wasp_reduced\")\n",
    "\n",
    "# Set the number of workers for data loading\n",
    "num_workers = None  # To manually set the number of workers, change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a2bb3",
   "metadata": {},
   "source": [
    "## 4. Exploring Data Augmentation Techniques\n",
    "\n",
    "Before training models with augmentation, let's visualize what different augmentation techniques do to our images. This helps us understand which transformations might be beneficial for our bee vs wasp classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bec8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's load some sample images to visualize augmentations\n",
    "data_module = helpers_01.load_display_data(\n",
    "    data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=False,  # We'll handle visualization ourselves\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "# Get a sample image to demonstrate augmentations\n",
    "val_loader = data_module.val_dataloader()\n",
    "sample_images, sample_labels = next(iter(val_loader))\n",
    "sample_image = sample_images[0]  # Take the first image\n",
    "\n",
    "# Convert back to PIL Image for easier augmentation demonstration\n",
    "# First denormalize the image\n",
    "denorm_image = sample_image * torch.tensor([0.229, 0.224, 0.225]).view(\n",
    "    3, 1, 1\n",
    ") + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "denorm_image = torch.clamp(denorm_image, 0, 1)\n",
    "pil_image = to_pil_image(denorm_image)\n",
    "\n",
    "print(f\"Sample image shape: {sample_image.shape}\")\n",
    "print(f\"Sample image label: {sample_labels[0]}\")\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(pil_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1986c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define various augmentation transforms\n",
    "augmentations = {\n",
    "    \"Original\": transforms.Compose([]),\n",
    "    \"Horizontal Flip\": transforms.RandomHorizontalFlip(p=1.0),\n",
    "    \"Rotation (±15°)\": transforms.RandomRotation(degrees=15),\n",
    "    \"Color Jitter\": transforms.ColorJitter(\n",
    "        brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1\n",
    "    ),\n",
    "    \"Random Crop\": transforms.RandomResizedCrop(size=(80, 80), scale=(0.8, 1.0)),\n",
    "    \"Gaussian Blur\": transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "}\n",
    "\n",
    "# Create a grid to show all augmentations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, transform) in enumerate(augmentations.items()):\n",
    "    if name == \"Original\":\n",
    "        aug_image = pil_image\n",
    "    else:\n",
    "        # Apply augmentation\n",
    "        aug_image = transform(pil_image)\n",
    "\n",
    "    axes[idx].imshow(aug_image)\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Data Augmentation Techniques\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231f0cf",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Augmentation Strengths\n",
    "\n",
    "Now let's create data loaders with different levels of augmentation and see how they affect the data. We have four augmentation strengths available:\n",
    "\n",
    "- **None**: No augmentation (baseline)\n",
    "- **Light**: Basic horizontal flips and small rotations\n",
    "- **Medium**: Adds color jitter and random crops\n",
    "- **Heavy**: Aggressive augmentation with blur and larger transformations\n",
    "\n",
    "One important thing the think about is how augmentations might impact the model's ability to learn about your data. For example, if you wanted a model to distinguish 'b' and 'd', horizontal flips would be a problem! Similarly, if color is important to the categories (as it may be in our bee vs wasp example), be careful with using color jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed635b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's compare different augmentation strengths side by side\n",
    "augmentation_levels = [\"none\", \"light\", \"medium\", \"heavy\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(augmentation_levels), 4, figsize=(16, 12))\n",
    "\n",
    "for row, aug_level in enumerate(augmentation_levels):\n",
    "    # Create data module with specific augmentation level\n",
    "    aug_data_module = helpers_01.load_display_data_augmented(\n",
    "        data_path,\n",
    "        batch_size=32,\n",
    "        shape=(80, 80, 3),\n",
    "        show_pictures=False,\n",
    "        train_split=0.8,\n",
    "        num_workers=num_workers,\n",
    "        augmentation_strength=aug_level,\n",
    "    )\n",
    "\n",
    "    # Get training images\n",
    "    train_loader = aug_data_module.train_dataloader()\n",
    "    images, labels = next(iter(train_loader))\n",
    "\n",
    "    # Show first 4 images for this augmentation level\n",
    "    for col in range(4):\n",
    "        img = images[col].numpy().transpose(1, 2, 0)\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        axes[row, col].imshow(img)\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(\n",
    "                f\"{aug_level.title()}\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Comparison of Augmentation Strengths\", fontsize=16)\n",
    "plt.suptitle(\"Augmentation levels = ['none', 'light', 'medium', 'heavy']\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60617aac",
   "metadata": {},
   "source": [
    "## 6. Training Models with Data Augmentation\n",
    "\n",
    "Now let's train models with different augmentation strengths and compare their performance. We'll start with a baseline model (no augmentation) and then experiment with different levels of augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7201a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Train baseline model (no augmentation)\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BASELINE MODEL (NO AUGMENTATION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_data_module = helpers_01.load_display_data_augmented(\n",
    "    data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=False,\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    augmentation_strength=\"none\",\n",
    ")\n",
    "\n",
    "baseline_model, baseline_trainer = helpers_01.train_model(\n",
    "    data_module=baseline_data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,  # Some dropout for regularization\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline model evaluation:\")\n",
    "baseline_results = helpers_01.test_model(\n",
    "    baseline_data_module, baseline_model, baseline_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c12bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Train model with light augmentation\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH LIGHT AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "light_data_module = helpers_01.load_display_data_augmented(\n",
    "    data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=False,\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    augmentation_strength=\"light\",\n",
    ")\n",
    "\n",
    "light_model, light_trainer = helpers_01.train_model(\n",
    "    data_module=light_data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nLight augmentation model evaluation:\")\n",
    "light_results = helpers_01.test_model(light_data_module, light_model, light_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Train model with medium augmentation\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH MEDIUM AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "medium_data_module = helpers_01.load_display_data_augmented(\n",
    "    data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=False,\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    augmentation_strength=\"medium\",\n",
    ")\n",
    "\n",
    "medium_model, medium_trainer = helpers_01.train_model(\n",
    "    data_module=medium_data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nMedium augmentation model evaluation:\")\n",
    "medium_results = helpers_01.test_model(medium_data_module, medium_model, medium_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9143b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Train model with heavy augmentation\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH HEAVY AUGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "heavy_data_module = helpers_01.load_display_data_augmented(\n",
    "    data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=False,\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    augmentation_strength=\"heavy\",\n",
    ")\n",
    "\n",
    "heavy_model, heavy_trainer = helpers_01.train_model(\n",
    "    data_module=heavy_data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    input_shape=(3, 80, 80),\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nHeavy augmentation model evaluation:\")\n",
    "heavy_results = helpers_01.test_model(heavy_data_module, heavy_model, heavy_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56531c",
   "metadata": {},
   "source": [
    "## 7. Comparing Results\n",
    "\n",
    "Let's create a summary comparison of all our models to see how data augmentation affected performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf80c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract test accuracies from each model\n",
    "models_summary = {\n",
    "    \"Baseline (No Aug)\": baseline_results[0][\"test_acc\"],\n",
    "    \"Light Augmentation\": light_results[0][\"test_acc\"],\n",
    "    \"Medium Augmentation\": medium_results[0][\"test_acc\"],\n",
    "    \"Heavy Augmentation\": heavy_results[0][\"test_acc\"],\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Model':<20} {'Test Accuracy':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for model_name, accuracy in models_summary.items():\n",
    "    print(f\"{model_name:<20} {accuracy:<15.3f}\")\n",
    "\n",
    "# Create a bar plot comparing accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(models_summary.keys())\n",
    "accuracies = list(models_summary.values())\n",
    "\n",
    "bars = plt.bar(\n",
    "    model_names, accuracies, color=[\"red\", \"orange\", \"lightblue\", \"darkblue\"]\n",
    ")\n",
    "plt.title(\"Model Performance Comparison: Effect of Data Augmentation\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.xlabel(\"Augmentation Strategy\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{acc:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6593de-22f0-4583-8b78-3c6af9b942e4",
   "metadata": {},
   "source": [
    "### KEY OBSERVATIONS:\n",
    "\n",
    "1. Compare validation vs training accuracy in each model's plots above\n",
    "2. Look for reduced overfitting with augmentation (smaller gap between train/val)\n",
    "3. Optimal augmentation level balances regularization without hurting performance\n",
    "4. Heavy augmentation might hurt performance if it's too aggressive\n",
    "5. Data augmentation is most effective when you have limited training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe424b",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Best Practices\n",
    "\n",
    "### What We Learned About Data Augmentation:\n",
    "\n",
    "1. **Reduces Overfitting**: Data augmentation helps models generalize better by exposing them to variations of the training data.\n",
    "\n",
    "2. **Augmentation Strength Matters**: \n",
    "   - **Light** augmentation often provides good benefits with minimal risk\n",
    "   - **Medium** augmentation can be very effective for most datasets\n",
    "   - **Heavy** augmentation may hurt performance if too aggressive\n",
    "\n",
    "3. **Dataset Size Impact**: Data augmentation is most beneficial when you have limited training data.\n",
    "\n",
    "4. **Training Time**: Models with augmentation may take longer to converge since they see more varied data.\n",
    "\n",
    "### Best Practices for Data Augmentation:\n",
    "\n",
    "- **Start with light augmentation** and gradually increase if needed\n",
    "- **Preserve class semantics** - don't apply transformations that change the class\n",
    "- **Use domain knowledge** - apply transformations that make sense for your data\n",
    "- **Monitor overfitting** - augmentation should reduce the train/validation gap\n",
    "- **Experiment systematically** - try different combinations and strengths\n",
    "\n",
    "### When to Use Data Augmentation:\n",
    "\n",
    "✅ **Good for:**\n",
    "- Small datasets\n",
    "- High overfitting\n",
    "- Need for robustness\n",
    "- Computer vision tasks\n",
    "\n",
    "❌ **Be careful with:**\n",
    "- Already large datasets\n",
    "- Time-sensitive applications\n",
    "- Tasks where transformations change meaning\n",
    "\n",
    "Data augmentation is a powerful tool in the computer vision toolkit. Combined with other regularization techniques like dropout, it can significantly improve model performance and generalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101020c9",
   "metadata": {},
   "source": [
    "----\n",
    "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
    "\n",
    " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
    "\n",
    "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
