{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data Imbalance in Computer Vision\n",
    "\n",
    "In this notebook, we will explore techniques to handle data imbalance using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import helpers_01\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Create a weighted sampler\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m [class_weights[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m]\n\u001b[1;32m     42\u001b[0m sampler \u001b[38;5;241m=\u001b[39m WeightedRandomSampler(sample_weights, \u001b[38;5;28mlen\u001b[39m(sample_weights))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "# Define a custom dataset class\n",
    "class BeeWaspDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = os.listdir(data_path)\n",
    "        for idx, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(data_path, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                self.images.append(os.path.join(class_path, img_name))\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((80, 80)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'data/bee_vs_wasp'\n",
    "dataset = BeeWaspDataset(data_path, transform=transform)\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, stratify=dataset.labels)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(dataset.labels), y=dataset.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Create a weighted sampler\n",
    "sample_weights = [class_weights[label] for label in train_data.labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "class BeeWaspDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = os.listdir(data_path)\n",
    "        for idx, class_name in enumerate(self.class_names):\n",
    "            class_path = os.path.join(data_path, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                self.images.append(os.path.join(class_path, img_name))\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((80, 80)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'data/bee_vs_wasp'\n",
    "dataset = BeeWaspDataset(data_path, transform=transform)\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, stratify=dataset.labels)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(dataset.labels), y=dataset.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Create a weighted sampler\n",
    "sample_weights = [class_weights[label] for label in train_data.labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = helpers_01.make_model()\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "model = helpers_01.compile_train_model(train_loader, val_loader, model, loss_fn, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "helpers_01.evaluate_model(val_loader, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matt's Demo Pytorch",
   "language": "python",
   "name": "pytorch_demo_2025"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
