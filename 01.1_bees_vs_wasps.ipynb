{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Concepts\n",
    "\n",
    "You may recall *Practicum AI*'s heroine Amelia, the AI-savvy nutritionist. At the end of our [*Deep Learning Foundations* course](https://practicumai.org/courses/deep_learning/), Amelia was helping with a computer vision project. Her colleague, an entomologist named Kevin, had a dataset of images of bees and wasps and wanted to classify them.\n",
    "\n",
    "**For this exercise, we have made a subsample of those data to make training easier.**\n",
    "\n",
    "![Image of bees and wasps from the dataset cover image](https://github.com/PracticumAI/deep_learning/blob/main/images/bees_wasps_dataset-cover.png?raw=true)\n",
    "\n",
    "\n",
    "## AI Pathway review for Bees vs Wasps\n",
    "\n",
    "If you have taken our [*Getting Started with AI* course](https://practicumai.org/courses/getting_started/), you may remember this figure of the **AI Application Development Pathway**. Let's take a quick review of how we applied this pathway in the case of the Bees vs Wasps example.\n",
    "\n",
    "![AI Application Development Pathway image showing the 7 steps in developing an AI application](https://practicumai.org/getting_started/images/application_dev_pathway.png)\n",
    "\n",
    "1. **Choose a problem to solve:** In this example, we need to classify images as bees, wasps, other insects, or a non-insect. \n",
    "2. **Gather data:** The data for the example comes from [Kaggle](https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp), a great repository of datasets, code, and models. Again, we have subsampled the full dataset to make training times more reasonable for the exercises and for most notebooks that deal with the dataset imbalance issues. We will explore the data imbalance problem in [notebook 01.4_data_imbalance.ipynb](01.4_data_imbalance.ipynb).\n",
    "3. **Clean and prepare the data:** In the *Deep Learning Foundations* course, we assumed that this was done for us. One issue that we ran into was that of class imbalance. There are many more images in some classes than others, leading to a poorly performing model. For most exercises, we have created a smaller, balanced dataset.\n",
    "4. **Choose a model:** In the *Deep Learning Foundations* course, we presented the model with little detail. Now that we know more about Convolutional Neural Networks (CNNs) and some other tools at our disposal, we will explore the model in more detail.\n",
    "   * As part of the iterative process of training models, one thing we noticed is that most of our models were **overfitting** — performing better on the training data than they did on the testing data. Essentially, the models memorized the training data but did not generalize well to new data that had not been seen. \n",
    "      * In this notebook, we will explore **dropout** as one mechanism to mitigate overfitting.\n",
    "5. **Train the model:** In training the model, we may have had a few issues. With so many hyperparameters to tune, it's easy to lose track of what combinations have been tried and how changes impacted model performance. \n",
    "   * In this notebook, we introduce you to [TensorBoard](https://www.tensorflow.org/tensorboard), a popular tool in a class of tools known as **experiment tracking** or **MLOps (Machine learning operations) tools**. These tools help track changes to hyperparameters, the training process, and the data. They allow comparison among runs and can even automate multiple runs for you. Learning to use MLOps tools will help you as you continue to learn more about AI workflows.\n",
    "6. **Evaluate the model:** We will continue to assess how the model performs on the validation set and adjust the model and hyperparameters to attempt to produce a better model.\n",
    "7. **Deploy the model:** We won't get to this stage in this exercise, but hopefully, we will end up with a model that could be deployed and achieve relatively good accuracy at solving the problem.\n",
    "\n",
    "\n",
    "## PyTorch and PyTorch Lightning\n",
    "\n",
    "Since the introduction of the *Practicum AI* program, the AI landscape has shifted significantly (no real surprise there!!). While [TensorFlow](https://www.tensorflow.org/) seemed like a good choice when we started making courses in 2021, and it can certainly be easier to get started with, the reality is that [Pytorch](https://pytorch.org/) has gained much more popularity. At this point, the *Practicum AI* team has made the decision to transition our code to use PyTorch. \n",
    "\n",
    "### PyTorch Lightning \n",
    "\n",
    "<img src='images/Lightning_logo.svg' alt=\"PyTorch Lightning logo\" width=\"20%\" align=\"right\">\n",
    "\n",
    "[PyTorch Lightning](https://lightning.ai/) is an open-source framework built on top of PyTorch that makes training deep learning models more straightforward. It abstracts many common tasks like managing training loops, logging, checkpointing, and handling hardware setups, allowing you to focus on the core aspects of your model and experimentation. \n",
    "\n",
    "Rather than writing repetitive code, you define key methods—such as `training_step` and `validation_step`—to describe the model's behavior while the Lightning trainer automates optimization details, synchronization, and even distributed training. This separation between scientific code and engineering routines leads to cleaner, more maintainable projects that are easier to scale.\n",
    "\n",
    "Additionally, PyTorch Lightning integrates smoothly with popular tools such as [TensorBoard](https://www.tensorflow.org/tensorboard), which simplifies tracking experiment metrics and visualizing performance. Overall, Lightning streamlines the training workflow, boosts reproducibility, and helps both beginners and seasoned researchers concentrate on innovation, not boilerplate coding.\n",
    "\n",
    "This course will make use of Lightning to simplify training.\n",
    "\n",
    "The Beginner Series of courses has also be updated to have a PytTorch version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of this notebook\n",
    "\n",
    "This notebook covers a fair bit of ground. To orient you, here's an outline of the topics covered. Note that you can also open the notebook outline to see section headers.\n",
    "\n",
    "1. Run through loading the data and exploring it a bit ([sections 1](#1.-Import-the-libraries-we-will-use) through 5).\n",
    "1. Set initial hyperparameters, train a CNN model, and evaluate the performance ([sections 6](#6.-Train-the-Model) through 7).\n",
    "1. Explore Tensorboard as a tool to gain more insight into model performance ([section 8](#8.-View-training-metrics-in-TensorBoard)).\n",
    "1. Summarize the results obtained so far ([section 9](#9.-Summary-so-far))\n",
    "1. Explore what a convolutional kernel is in more detail, visualizing kernels and convolved images ([section 10](#10.-A-look-inside-CNNs))\n",
    "1. Add dropout to our model ([section 11](#11.-Dropout)) and discuss the padding and stride hyperparameters ([section 12](#12.-Padding-and-stride-for-convolutional-layers))\n",
    "1. Experiment with hyperparameters ([section 13](#13.-Experimenting-with-Hyperparameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Many functions are moved to helpers_01.py to keep this file clean.\n",
    "import helpers_01\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check PyTorch installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Pytorch versions and check for GPU\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f'  Should be \"True\" if Pytorch was built for GPU: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Available GPU: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"  No GPU available, will use CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting the data\n",
    "\n",
    "For details about the dataset and the code used to get the data, you can look at the [helpers_01.py file](helpers_01.py). \n",
    "\n",
    "If you need to download the data, it is [hosted for public download from HiPerGator as a `tar.gz` file](https://data.rc.ufl.edu/pub/practicum-ai/Computer_Vision/bee_vs_wasp_reduced.tar.gz). If you need to manually extract the data, you can add a cell and run: `helpers_01.extract_file(\"bee_vs_wasp_reduced.tar.gz\", \"data\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the data.\n",
    "# This will look for the data files required for this notebook in some common locations.\n",
    "#   * If it can't find the data, it will ask if you know where it is.\n",
    "#   * If you do, answer yes and provide the path to the data\n",
    "#       (up to and including the `bee_vs_wasp` folder name).\n",
    "#   * If not, it will ask if you want to download it.\n",
    "#      * If you answer yes, it will download the data and\n",
    "#        extract it into your data folder.\n",
    "\n",
    "data_path = helpers_01.manage_data(\n",
    "    folder_name=\"bee_vs_wasp_reduced\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting the number of workers\n",
    "\n",
    "In machine learning, especially when working with large datasets, it is often necessary to load data in parallel to speed up the training process. This is where the concept of \"workers\" comes into play. Each worker is a process that can load data independently, allowing for faster data preparation and feeding into the model.\n",
    "\n",
    "One of the really great things about PyTorch Lightning is that it can efficiently use multiple CPUs (or cores) using these worker processes. This can be important in keeping the GPU fed with data rather than sitting idle waiting for data.\n",
    "\n",
    "Up to a certain point, more workers will help keep the GPU fed with data. Requesting more workers than the number of available CPU cores will hurt performance, however.\n",
    "\n",
    "The cell below will set the number of workers. The options here are:\n",
    "- Using a manually set number (change `None` in the first line of code to an integer)\n",
    "- Using a number from the Slurm scheduler. Slurm is the software that manages the compute load for many high-performance computer clusters, such as UF's HiPerGator.\n",
    "- Using the number of CPU cores on the computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of workers to use for data loading\n",
    "num_workers = None  # To manually set the number of workers, change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine some images\n",
    "\n",
    "Many of the steps in this notebook are written as functions, making it easier to run these steps repeatedly as you work on optimizing the various hyperparameters.\n",
    "\n",
    "The `helpers_01.load_display_data()` function takes: \n",
    "* A path to the data: set from above.\n",
    "* The batch size: set as 32 below, but a good hyperparameter to tune.\n",
    "* Target shape for images: set as 80x80 color images below, another possible hyperparameter.\n",
    "* Whether or not to show sample images.\n",
    "* The train/validation split\n",
    "* The number of workers\n",
    "\n",
    "The function returns training and validation datasets. To help highlight the class imbalance issue, the function has been updated to report the number of images and the percentage of the total in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Lightning DataModule from helpers_01\n",
    "data_module = helpers_01.load_display_data(\n",
    "    data_path,\n",
    "    batch_size=32,\n",
    "    shape=(80, 80, 3),\n",
    "    show_pictures=True,\n",
    "    train_split=0.8,  # 80% train, 20% validation\n",
    "    num_workers=num_workers,  # Number of workers for data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Using PyTorch Lightning's standard patterns, we create a LightningModule that defines:\n",
    "\n",
    "- **Model architecture** in `__init__` and `forward`\n",
    "- **Training logic** in `training_step` \n",
    "- **Validation logic** in `validation_step`\n",
    "- **Optimizer configuration** in `configure_optimizers`\n",
    "- **Built-in metrics** using Lightning's torchmetrics integration\n",
    "\n",
    "The training process is handled by Lightning's `Trainer` which provides:\n",
    "- Automatic GPU/CPU handling\n",
    "- Built-in logging to TensorBoard\n",
    "- Model checkpointing and early stopping\n",
    "- Progress bars and model summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using standard PyTorch Lightning approach\n",
    "model, trainer = helpers_01.train_model(\n",
    "    data_module=data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",  # Lightning will choose GPU if available\n",
    "    devices=\"auto\",  # Lightning will choose optimal device count\n",
    "    input_shape=(3, 80, 80),  # Match the shape used in data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model\n",
    "\n",
    "The `test_model` function will now provide a comprehensive evaluation, including:\n",
    "- Test accuracy and loss metrics\n",
    "- Training and validation loss curves over epochs\n",
    "- Training and validation accuracy curves over epochs  \n",
    "- Confusion matrix showing prediction accuracy per class\n",
    "- Per-class precision, recall, and F1 scores\n",
    "\n",
    "These visualizations help us understand:\n",
    "- **Loss curves**: Whether the model is overfitting (validation loss increases while training loss decreases)\n",
    "- **Accuracy curves**: How well the model generalizes to unseen data\n",
    "- **Confusion matrix**: Which classes the model confuses with each other\n",
    "- **Per-class metrics**: How well the model performs on each individual class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model and display comprehensive evaluation plots\n",
    "# The debug output will show us what TensorBoard metrics are actually available\n",
    "test_results = helpers_01.test_model(data_module, model, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View training metrics in TensorBoard\n",
    "\n",
    "Now that we've run one training cycle, we can open TensorBoard and have a look at the visualizations it provides to evaluate training performance.\n",
    "\n",
    "The detailed instructions for different platforms are in the course content. In general, we use the `tensorboard --logdir ./logs` command to start TensorBoard and then connect in a Web browser. Here's a screenshot of what that might look like:\n",
    "\n",
    "![Screenshot of the TensorBoard web page](images/tensorboard_screenshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View TensorBoard logs\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary so far\n",
    "\n",
    "A key point that we can take away so far is:\n",
    "\n",
    "* Most of the time, our model struggles to do better than about 70% accuracy on the validation data. Accuracy on the training data is closer to 90%. This suggests that our model is overfit to the training data.\n",
    "\n",
    "Before we move on to working more to improve the model, let's take a quick look at the inner workings of the convolutional kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. A look inside CNNs\n",
    "To get an idea for what is happening *inside* this model, let's look at a **feature map**. Below we see a vertical edge detection filter applied to a sunflower picture, resulting in a feature map of that image.\n",
    "\n",
    "![Two pictures of a sunflower. On the left is the original image, on the right is the convolved image resulting from applying an edge detecting convolutional filter.](images/filtered_sunflower_nb01.jpg)\n",
    "\n",
    "Imagine you're a detective investigating a scene.  A feature map is like a sketch you create, focusing on specific details that might be clues to solving the case.  In a CNN, the \"case\" is recognizing patterns in an image, and the feature maps capture these patterns at different levels of complexity. Early layers might create feature maps that detect basic edges, corners, or blobs of color. As the network progresses through more layers, the feature maps become more intricate, combining these simpler features to represent more complex objects or shapes.\n",
    "\n",
    "Getting a bit more technical, a feature map is a 2D array of activations produced by applying a convolutional filter to an input image or a previous layer's feature map. It essentially captures the presence and strength of specific visual features that the filter is optimized to detect within the input.\n",
    "\n",
    "The **convolutional filters** (also just called \"filters\" or \"kernels\") are small matrices containing learnable weights. The filter \"slides\" across the input image, performing element-wise multiplication with the underlying image data at each position. The results of the multiplications are summed and then passed through an activation function (like ReLU) to introduce non-linearity and help the network learn complex features. A convolutional layer typically has multiple filters, each generating a separate feature map. These feature maps capture different aspects of the input, providing a richer representation of the image.\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "> The above sunflower example could potentially be a bit misleading. While a model *might* develop a vertical edge detection filter, the model develops its filters' weights through the same backpropagation process as other deep neural networks. Most of the filters and their resulting feature maps will not be as easily interpretable as the vertical edge detection filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filters from the first convolutional layer of the PyTorch model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "filters = (\n",
    "    model.conv1.weight.data.cpu().numpy()\n",
    ")  # Shape: (out_channels, in_channels, height, width)\n",
    "\n",
    "# Get the first batch of images from the validation set\n",
    "val_loader = data_module.val_dataloader()\n",
    "images, labels = next(iter(val_loader))\n",
    "\n",
    "# Take only the first image for visualization\n",
    "sample_image = images[0:1]  # Keep batch dimension\n",
    "sample_label = labels[0]\n",
    "\n",
    "# Ensure the input tensor is on the same device as the model\n",
    "device = next(model.parameters()).device\n",
    "sample_image = sample_image.to(device)\n",
    "\n",
    "# Get feature maps from the first conv layer\n",
    "with torch.no_grad():\n",
    "    # Forward pass through first conv + relu + pool\n",
    "    x = model.relu(model.conv1(sample_image))\n",
    "    feature_maps_conv1 = x.cpu().numpy()\n",
    "    feature_maps_pool1 = model.pool(x).cpu().numpy()\n",
    "\n",
    "# Convert sample image to numpy for display\n",
    "sample_image_np = sample_image[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Normalize the filters and feature maps for better visualization\n",
    "# PyTorch filters have shape (out_channels, in_channels, height, width)\n",
    "# We'll take the first 3 filters and visualize them\n",
    "num_filters = min(3, filters.shape[0])\n",
    "normal_filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# ----- Display First 3 Filters -----\n",
    "for i in range(num_filters):\n",
    "    # Get the i-th filter (shape: in_channels, height, width)\n",
    "    filter_weights = normal_filters[i]  # Shape: (3, 3, 3) for RGB input\n",
    "\n",
    "    # Create RGB visualization by treating channels as RGB\n",
    "    if filter_weights.shape[0] == 3:  # RGB input\n",
    "        # Rearrange from (C, H, W) to (H, W, C) for display\n",
    "        filter_rgb = np.transpose(filter_weights, (1, 2, 0))\n",
    "        # Normalize to [0, 1] range\n",
    "        filter_rgb = (filter_rgb - filter_rgb.min()) / (\n",
    "            filter_rgb.max() - filter_rgb.min()\n",
    "        )\n",
    "    else:\n",
    "        # For non-RGB, just show the first channel\n",
    "        filter_rgb = filter_weights[0]\n",
    "\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    if len(filter_rgb.shape) == 3:\n",
    "        plt.imshow(filter_rgb)\n",
    "    else:\n",
    "        plt.imshow(filter_rgb, cmap=\"gray\")\n",
    "    plt.title(f\"Filter {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# ----- Original Image -----\n",
    "plt.subplot(3, 3, 5)  # Center position\n",
    "# Normalize image for display (assuming it's normalized for training)\n",
    "img_display = sample_image_np.copy()\n",
    "# If image was normalized during training, denormalize it\n",
    "img_display = (img_display - img_display.min()) / (\n",
    "    img_display.max() - img_display.min()\n",
    ")\n",
    "plt.imshow(img_display)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# ----- Feature Maps from First Conv Layer -----\n",
    "num_feature_maps = min(3, feature_maps_conv1.shape[1])\n",
    "for i in range(num_feature_maps):\n",
    "    plt.subplot(3, 3, i + 7)  # Bottom row\n",
    "    feature_map = feature_maps_conv1[0, i, :, :]  # Get i-th feature map\n",
    "    plt.imshow(feature_map, cmap=\"gray\")\n",
    "    plt.title(f\"Feature Map {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Visualizing PyTorch CNN - Filters and Feature Maps\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some information about the shapes\n",
    "print(f\"Filter weights shape: {filters.shape}\")\n",
    "print(f\"Sample image shape: {sample_image.shape}\")\n",
    "print(f\"Feature maps after conv1 shape: {feature_maps_conv1.shape}\")\n",
    "print(f\"Sample image label: {sample_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned weights for each of the color channel pixel intensity values of the filters above are printed inside their rendered cells (the weights for the red channel are in red, green in green, and blue in blue). The colors of the filters are shown for illustrative purposes, it's the weights that matter!\n",
    "\n",
    "As mentioned above, the feature maps you see may not be as easily interpretable as the edge detection filter to us, but they *are* useful to the model. They help the model learn to recognize patterns in the images. You can rerun the cell above to see the feature maps for different images.\n",
    "\n",
    "Below, we'll look at a few more feature maps, this time from the first, second, and third convolutional layers of our model. This will give us an idea of what the model is learning at different levels of abstraction. In the first Module we explained that each layer looks at a larger area of the image, so the feature maps from the first layer will be more detailed than those from the second layer, and so on.\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "> If you have changed the model architecture, you may need to adjust the layer numbers below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature maps from different layers of the PyTorch model\n",
    "model.eval()\n",
    "\n",
    "# Get the first batch of images from the validation set\n",
    "val_loader = data_module.val_dataloader()\n",
    "images, labels = next(iter(val_loader))\n",
    "\n",
    "# Take only the first image for visualization\n",
    "sample_image = images[0:1]  # Keep batch dimension\n",
    "\n",
    "# Ensure the input tensor is on the same device as the model\n",
    "device = next(model.parameters()).device\n",
    "sample_image = sample_image.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get feature maps from first conv layer (after ReLU and pooling)\n",
    "    x1 = model.relu(model.conv1(sample_image))\n",
    "    feature_maps_conv1 = model.pool(x1).cpu().numpy()\n",
    "\n",
    "    # Get feature maps from second conv layer (after ReLU and pooling)\n",
    "    x2 = model.relu(model.conv2(x1))\n",
    "    feature_maps_conv2 = model.pool(x2).cpu().numpy()\n",
    "\n",
    "    # The \"last\" layer would be the final conv layer (conv2 in our simple model)\n",
    "    # So we'll use conv2 as both middle and last for demonstration\n",
    "    feature_maps_last = feature_maps_conv2\n",
    "\n",
    "# Convert sample image to numpy for display\n",
    "sample_image_np = sample_image[0].permute(1, 2, 0).cpu().numpy()\n",
    "sample_image_np = (sample_image_np - sample_image_np.min()) / (\n",
    "    sample_image_np.max() - sample_image_np.min()\n",
    ")\n",
    "\n",
    "# Select random feature maps to display\n",
    "num_filters_conv1 = feature_maps_conv1.shape[1]\n",
    "num_filters_conv2 = feature_maps_conv2.shape[1]\n",
    "\n",
    "# Ensure we don't try to select more indices than available\n",
    "random_indices_conv1 = random.sample(\n",
    "    range(num_filters_conv1), min(3, num_filters_conv1)\n",
    ")\n",
    "random_indices_conv2 = random.sample(\n",
    "    range(num_filters_conv2), min(3, num_filters_conv2)\n",
    ")\n",
    "random_indices_last = random_indices_conv2  # Same as conv2 for our simple model\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "# ----- Original Image -----\n",
    "plt.subplot(4, 3, 2)  # Position 2 in top row\n",
    "plt.imshow(sample_image_np)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# ----- First Layer Feature Maps (Conv1 + Pool) -----\n",
    "for i, idx in enumerate(random_indices_conv1):\n",
    "    plt.subplot(4, 3, i + 4)  # Second row: positions 4, 5, 6\n",
    "    feature_map = feature_maps_conv1[0, idx, :, :]\n",
    "    plt.imshow(feature_map, cmap=\"gray\")\n",
    "    plt.title(f\"Conv1 Feature Map {idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# ----- Second Layer Feature Maps (Conv2 + Pool) -----\n",
    "for i, idx in enumerate(random_indices_conv2):\n",
    "    plt.subplot(4, 3, i + 7)  # Third row: positions 7, 8, 9\n",
    "    feature_map = feature_maps_conv2[0, idx, :, :]\n",
    "    plt.imshow(feature_map, cmap=\"gray\")\n",
    "    plt.title(f\"Conv2 Feature Map {idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# ----- \"Last\" Layer Feature Maps (same as Conv2 for our simple model) -----\n",
    "for i, idx in enumerate(random_indices_last):\n",
    "    plt.subplot(4, 3, i + 10)  # Fourth row: positions 10, 11, 12\n",
    "    feature_map = feature_maps_last[0, idx, :, :]\n",
    "    plt.imshow(feature_map, cmap=\"gray\")\n",
    "    plt.title(f\"Final Feature Map {idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"PyTorch CNN - Feature Maps Across Layers\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about the feature map shapes\n",
    "print(f\"Original image shape: {sample_image.shape}\")\n",
    "print(f\"Conv1 feature maps shape: {feature_maps_conv1.shape}\")\n",
    "print(f\"Conv2 feature maps shape: {feature_maps_conv2.shape}\")\n",
    "print(f\"Selected Conv1 feature map indices: {random_indices_conv1}\")\n",
    "print(f\"Selected Conv2 feature map indices: {random_indices_conv2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the other hyperparameters in Section 5 above, the number of filters, the size of the filters, and the stride of the filters are all hyperparameters that can be adjusted. You can also add or remove convolutional and pooling layers, or add dropout layers.\n",
    "\n",
    "\n",
    "## 11. Dropout\n",
    "\n",
    "Dropout layers are a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training. In PyTorch, you can add dropout using:\n",
    "    \n",
    "```python\n",
    "   nn.Dropout(0.5)  # 50% dropout rate (which is actually higher than generally used)\n",
    "```\n",
    "\n",
    "## 12. Padding and stride for convolutional layers\n",
    "\n",
    "In PyTorch, you can adjust the stride and padding of convolutional layers using the `stride` and `padding` arguments in `nn.Conv2d`. The `stride` parameter controls how much the filter moves at each step, while `padding` adds zeros (or other value) around the input. Here are some examples:\n",
    "\n",
    "```python\n",
    "    # Standard convolutional layer with padding=1 (keeps spatial dimensions roughly the same)\n",
    "    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    # Convolutional layer with stride=2 (reduces spatial dimensions by half)\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    # MaxPooling layer (typically no padding needed)\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "```\n",
    "\n",
    "The `train_model` function accepts `dropout_rate` and `conv_padding` parameters, allowing you to experiment with different configurations to reduce overfitting and control the spatial dimensions of your feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train a model with different dropout and padding settings\n",
    "\n",
    "# Example 1: High dropout to reduce overfitting\n",
    "print(\"Training model with high dropout (0.5) to reduce overfitting...\")\n",
    "model_dropout, trainer_dropout = helpers_01.train_model(\n",
    "    data_module=data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=10,  # Fewer epochs for demonstration\n",
    "    dropout_rate=0.5,  # Add a dropout rate\n",
    "    conv_padding=1,  # Standard padding\n",
    ")\n",
    "\n",
    "print(\"Model with dropout - check for reduced overfitting\")\n",
    "helpers_01.test_model(data_module, model_dropout, trainer_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Experimenting with Hyperparameters\n",
    "\n",
    "The example above demonstrates how to train a model with different hyperparameter configurations. You can experiment with these parameters:\n",
    "- **`dropout_rate`**: Try values from 0.0 (no dropout) to 0.8 (aggressive dropout)\n",
    "- **`conv_padding`**: Usually 0, 1, or 2 depending on desired output size\n",
    "\n",
    "Compare the training/validation curves and confusion matrices to see which configuration works best for reducing overfitting while maintaining good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's also try some extreme examples to see the effects more clearly\n",
    "\n",
    "# Example 2: Very conservative model (low dropout, minimal padding)\n",
    "print(\"Training conservative model (low dropout, minimal padding)...\")\n",
    "model_conservative, trainer_conservative = helpers_01.train_model(\n",
    "    data_module=data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=5,\n",
    "    dropout_rate=0.1,  # Very low dropout\n",
    "    conv_padding=0,  # No padding (valid convolution)\n",
    ")\n",
    "\n",
    "# Example 3: Very aggressive regularization\n",
    "print(\"\\nTraining highly regularized model...\")\n",
    "model_aggressive, trainer_aggressive = helpers_01.train_model(\n",
    "    data_module=data_module,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=5,\n",
    "    dropout_rate=0.8,  # Very high dropout\n",
    "    conv_padding=2,  # High padding\n",
    ")\n",
    "\n",
    "print(\"\\nConservative model results:\")\n",
    "helpers_01.test_model(data_module, model_conservative, trainer_conservative)\n",
    "\n",
    "print(\"\\nAggressive regularization model results:\")\n",
    "helpers_01.test_model(data_module, model_aggressive, trainer_aggressive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue experimenting with hyperparameters\n",
    "\n",
    "Spend some time experimenting and see how good a model you can get. Remember to check TensorBoard to make comparisons easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
    "\n",
    " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
    "\n",
    "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
