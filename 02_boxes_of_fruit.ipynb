{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Computer Vision Tasks\n",
    "\n",
    "Now that Kevin has a better understanding of how computer vision classification works, he needs to learn more about other computer vision tasks. His manager has asked him to move on from wasps and bees to... fruits and (bounding) boxes! Kevin thankfully has an annotated dataset already, so he can start learning about object detection.\n",
    "\n",
    "As before, the dataset was found on. [Check out the dataset information](https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/data)\n",
    "\n",
    "![Image of fruits and bounding boxes from the dataset cover image](notebook_images/fruits_detection_dataset-cover.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "# This notebook will be used for an Object Detection task that trains a model on the fruits_detection dataset using YOLOv8\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml \n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/fruits_detection\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSorry, I cannot find the data. Please download it manually from https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/ and unpack it to the datasets folder.\u001b[39m\u001b[38;5;124m'\u001b[39m)      \n\u001b[1;32m--> 101\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[43mmanage_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mmanage_data\u001b[1;34m(folder_name)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Try to find the data for the exercise and return the path'''\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Check common paths of where the data might be on different systems\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m likely_paths\u001b[38;5;241m=\u001b[39m [\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/blue/practicum-ai/share/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     41\u001b[0m                os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/project/scinet_workshop2/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     42\u001b[0m                os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m'\u001b[39m, folder_name),\n\u001b[0;32m     43\u001b[0m                os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(folder_name)]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m likely_paths:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def download_file(url=\"https://www.dropbox.com/scl/fi/ioupfqya76b7p8m1v1kdc/fruits_detection.zip?rlkey=ofgre83fdxa98p7ity8j9z8ip&st=atv7sz18&dl=1\", filename=\"fruits_detection.zip\"):\n",
    "                        \n",
    "    # Check to see if the datasets folder exists\n",
    "    if not os.path.exists(\"datasets\"):\n",
    "        os.makedirs(\"datasets\")\n",
    "    \n",
    "    # Download the file using requests\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Create a file object and write the response content in chunks\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Wait for the file to finish downloading\n",
    "    while not os.path.exists(filename):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Print a success message\n",
    "    print(f\"Downloaded {filename} successfully.\")\n",
    "\n",
    "def extract_file(filename, data_folder):\n",
    "    # Check if the file is a zip file\n",
    "    if zipfile.is_zipfile(filename):\n",
    "        # Open the zip file\n",
    "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "            # Extract all the files to the data folder\n",
    "            zip_ref.extractall(data_folder)\n",
    "            # Print a success message\n",
    "            print(f\"Extracted {filename} to {data_folder} successfully.\")\n",
    "    else:\n",
    "        # Print an error message\n",
    "        print(type(filename))\n",
    "        print(f\"{filename} is not a valid zip file.\")\n",
    "    \n",
    "def manage_data(folder_name='fruits_detection'):\n",
    "    '''Try to find the data for the exercise and return the path'''\n",
    "    \n",
    "    # Check common paths of where the data might be on different systems\n",
    "    likely_paths= [os.path.normpath(f'/blue/practicum-ai/share/data/{folder_name}'),\n",
    "                   os.path.normpath(f'/project/scinet_workshop2/data/{folder_name}'),\n",
    "                   os.path.join('datasets', folder_name),\n",
    "                   os.path.normpath(folder_name)]\n",
    "    \n",
    "    for path in likely_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f'Found data at {path}.')\n",
    "            return path\n",
    "\n",
    "    answer = input(f'Could not find data in the common locations. Do you know the path? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "        path = os.path.join(os.path.normpath(input('Please enter the path to the data folder: ')),folder_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f'Thanks! Found your data at {path}.')\n",
    "            return path\n",
    "        else:\n",
    "            print(f'Sorry, that path does not exist.')\n",
    "    \n",
    "    answer = input('Do you want to download the data? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "\n",
    "        ''' Check and see if the downloaded data is inside the .gitignore file, and adds them to the list of files to ignore if not. \n",
    "        This is to prevent the data from being uploaded to the repository, as the files are too large for GitHub.'''\n",
    "        \n",
    "        if os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'r') as f:\n",
    "                ignore = f.read().split('\\n')\n",
    "        # If the .gitignore file does not exist, create a new one\n",
    "        elif not os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'w') as f:\n",
    "                f.write('')\n",
    "            ignore = []\n",
    "        else:\n",
    "            ignore = []\n",
    "\n",
    "        # Check if the .gz file is in the ignore list\n",
    "        if 'fruits_detection.zip' not in ignore:\n",
    "            ignore.append('fruits_detection.zip')\n",
    "            \n",
    "        # Check if the data/ folder is in the ignore list\n",
    "        if 'datasets/' not in ignore:\n",
    "            ignore.append('datasets/')\n",
    "\n",
    "        # Write the updated ignore list back to the .gitignore file\n",
    "        with open('.gitignore', 'w') as f:\n",
    "            f.write('\\n'.join(ignore))\n",
    "\n",
    "        print(\"Updated .gitignore file.\")\n",
    "        print('Downloading data, this may take a minute.')\n",
    "        download_file()\n",
    "        print('Data downloaded, unpacking')\n",
    "        extract_file(\"fruits_detection.zip\", \"datasets\")\n",
    "        print('Data downloaded and unpacked. Now available at datasets/fruits_detection.')\n",
    "        return os.path.normpath('datasets/fruits_detection')   \n",
    "\n",
    "    print('Sorry, I cannot find the data. Please download it manually from https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/ and unpack it to the datasets folder.')      \n",
    "\n",
    "\n",
    "data_path = manage_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m         plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m     57\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 59\u001b[0m \u001b[43mexplore_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_picture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_hist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mexplore_data\u001b[1;34m(data_dir, show_picture, show_hist)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplore_data\u001b[39m(data_dir, show_picture\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_hist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Examine some sample images\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show_picture:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Get valid image folders \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         image_folders \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(data_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, f))] \n\u001b[0;32m     12\u001b[0m         sample_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign the path to the dataset\n",
    "data_dir = r\"datasets/fruits_detection\"\n",
    "\n",
    "# Make a histogram of the number of images in each class\n",
    "def explore_data(data_dir, show_picture=True, show_hist=True):\n",
    "    \n",
    "    # Examine some sample images\n",
    "    if show_picture:\n",
    "        # Get valid image folders \n",
    "        image_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))] \n",
    "\n",
    "        sample_images = []\n",
    "        for i in range(5):\n",
    "            folder = random.choice(image_folders) \n",
    "            img_path = os.path.join(data_dir, folder, 'images', random.choice(os.listdir(os.path.join(data_dir, folder, 'images'))))\n",
    "            sample_images.append(img_path)\n",
    "\n",
    "        # Plot the sample images\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Make a histogram of the number of images in each class\n",
    "    if show_hist:\n",
    "        def get_class_counts(folder_path):  # Change from data_dir to folder_path\n",
    "            class_counts = {}\n",
    "            labels_path = os.path.join(folder_path, 'labels')  # Add labels path\n",
    "            for filename in os.listdir(labels_path):  # Update listdir\n",
    "                with open(os.path.join(labels_path, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        class_id = int(line.split(' ')[0])  # Assuming labels are in YOLO format\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "            return class_counts\n",
    "\n",
    "        train_counts = get_class_counts(os.path.join(data_dir, 'train'))  # Add os.path.join\n",
    "        val_counts = get_class_counts(os.path.join(data_dir, 'valid'))\n",
    "        test_counts = get_class_counts(os.path.join(data_dir, 'test'))\n",
    "        class_names = ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']\n",
    "        num_classes = len(class_names)\n",
    "\n",
    "        data_counts = {\n",
    "            'train': pd.Series(train_counts),\n",
    "            'val': pd.Series(val_counts),\n",
    "            'test': pd.Series(test_counts)\n",
    "        }\n",
    "        df = pd.DataFrame(data_counts)\n",
    "\n",
    "        df.plot.bar(figsize=(10, 6))\n",
    "        plt.xlabel('Class Name')\n",
    "        plt.xticks(np.arange(num_classes), class_names)\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.title('Distribution of Images per Class')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "explore_data(data_dir, show_picture=True, show_hist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file created: fruits_detection_data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create a YAML file for the YOLOv8 model configuration\n",
    "\n",
    "def create_yaml(data_dir, class_names, yaml_file='fruits_detection_data.yaml'):\n",
    "    \"\"\"\n",
    "    Creates a YOLOv8 data.yaml file. YAML stands for \"YAML Ain't Markup Language\" and is a human-readable data serialization format.\n",
    "    A YAML file is used to define the dataset configuration for training a YOLOv8 model.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the dataset root directory.\n",
    "        class_names (list): List of class names.\n",
    "        yaml_file (str): Name of the YAML file to save. Defaults to 'data.yaml'.\n",
    "    \"\"\"\n",
    "\n",
    "    yaml_dict = {\n",
    "        # 'path': data_dir,  # Path to your dataset\n",
    "        'train': data_dir + '/train/images',  # Relative path to training images\n",
    "        'val': data_dir + '/valid/images',    # Relative path to validation images\n",
    "        'test': data_dir + '/test/images',    # Relative path to testing images\n",
    "\n",
    "        'num_classes': len(class_names),   # Number of classes\n",
    "        'names': class_names      # List of class names\n",
    "    }\n",
    "\n",
    "    with open(yaml_file, 'w') as outfile:\n",
    "        yaml.dump(yaml_dict, outfile, default_flow_style=False)\n",
    "\n",
    "    print(f'YAML file created: {yaml_file}')\n",
    "\n",
    "data_dir = 'fruits_detection'\n",
    "class_names = ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']\n",
    "\n",
    "create_yaml(data_dir, class_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.2 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.9 ðŸš€ Python-3.8.18 torch-2.3.0+cu121 CUDA:0 (NVIDIA A100-SXM4-80GB, 81051MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=fruits_detection_data.yaml, epochs=3, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train5\n",
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752482  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3012018 parameters, 3012002 gradients\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/i.lutticken/computer_vision/datasets/fruits_detection/train/labels... 7108 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7108/7108 [00:02<00:00, 3219.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/i.lutticken/computer_vision/datasets/fruits_detection/train/images/3d8be4f881b8c54c_jpg.rf.0d7b6d095459cece040b47b246d807af.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/i.lutticken/computer_vision/datasets/fruits_detection/train/images/3d8be4f881b8c54c_jpg.rf.64e869a9bedd5f012cc2a1129c6ca229.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/i.lutticken/computer_vision/datasets/fruits_detection/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.lutticken/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/i.lutticken/computer_vision/datasets/fruits_detection/valid/labels... 914 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 914/914 [00:00<00:00, 2336.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/i.lutticken/computer_vision/datasets/fruits_detection/valid/images/3d3ddc3054b32eb7_jpg.rf.03e7789aaf5212e2634b84ef502e0832.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/i.lutticken/computer_vision/datasets/fruits_detection/valid/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.lutticken/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3      2.84G      3.129      4.083       3.71         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 444/445 [01:08<00:00,  7.26it/s]/home/i.lutticken/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "        1/3      2.89G      3.127      4.082      3.708         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 445/445 [01:09<00:00,  6.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:08<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        914       3227      0.205     0.0446      0.014    0.00541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3      3.05G      2.093      3.262       2.53         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 445/445 [01:06<00:00,  6.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:07<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        914       3227      0.266      0.103     0.0522      0.025\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3      3.09G       1.76      2.859      2.153         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 445/445 [01:05<00:00,  6.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:06<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        914       3227      0.323      0.147     0.0792     0.0407\n",
      "\n",
      "3 epochs completed in 0.063 hours.\n",
      "Optimizer stripped from runs/detect/train5/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train5/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train5/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.9 ðŸš€ Python-3.8.18 torch-2.3.0+cu121 CUDA:0 (NVIDIA A100-SXM4-80GB, 81051MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3006818 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:08<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        914       3227      0.323      0.147      0.079     0.0407\n",
      "                 Apple        914        557      0.191      0.208      0.101     0.0609\n",
      "                Banana        914        390      0.163      0.041     0.0433     0.0155\n",
      "                 Grape        914        809      0.228      0.063     0.0414     0.0158\n",
      "                Orange        914       1100      0.169       0.39      0.174     0.0896\n",
      "             Pineapple        914        154          1          0      0.013    0.00513\n",
      "            Watermelon        914        217      0.186       0.18      0.101     0.0571\n",
      "Speed: 0.2ms preprocess, 0.7ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Make the YOLOv8 model\n",
    "model = YOLO('yolov8n.yaml')\n",
    "results = model.train(data='fruits_detection_data.yaml', imgsz=640, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "0.13.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
