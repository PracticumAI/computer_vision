{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"https://github.com/PracticumAI/practicumai.github.io/blob/84b04be083ca02e5c7e92850f9afd391fc48ae2a/images/icons/practicumai_computer_vision.png?raw=true\" alt=\"Practicum AI: Computer Vision icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data Imbalance with Transfer Learning\n",
    "\n",
    "<img src='images/data_imbalance_histogram.png' alt='Histogram of the data imbalance in the bees vs wasp dataset showing that there are many more images of wasps than other categories.' align='right'>\n",
    "\n",
    "Welcome to an advanced exploration of handling data imbalance! This notebook combines the power of [**transfer learning**](01.3_transfer_learning.ipynb) with **imbalance handling techniques** to tackle one of the most common challenges in real-world computer vision.\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Work with the **full** bee vs wasp dataset (highly imbalanced)\n",
    "2. Use **EfficientNetB0** pre-trained features for better performance (We'll use the smaller B0 model here to help speed training along)\n",
    "3. Apply multiple imbalance handling techniques with transfer learning\n",
    "5. Evaluate comprehensive metrics appropriate for imbalanced datasets\n",
    "\n",
    "### Why Transfer Learning + Imbalance Handling?\n",
    "\n",
    "Transfer learning is particularly powerful for imbalanced datasets because:\n",
    "- **Pre-trained features**: Rich representations learned from millions of images help even with limited minority class data\n",
    "- **Faster convergence**: Models train faster, allowing more experimentation with imbalance techniques\n",
    "- **Better generalization**: Pre-trained features often generalize better to minority classes\n",
    "- **Reduced overfitting**: Especially important when some classes have very few examples\n",
    "\n",
    "### The Challenge: Real-World Data Imbalance\n",
    "\n",
    "Real datasets are messy! The full bee vs wasp dataset contains:\n",
    "- **Thousands** of images for some classes\n",
    "- **Hundreds** for others\n",
    "- Natural distribution that reflects real-world collection challenges\n",
    "\n",
    "This creates the perfect testbed for combining advanced model architectures with imbalance handling.\n",
    "\n",
    "### The Lazy Student\n",
    "\n",
    "One way people explain data imbalance is using an analogy to a lazy student. Imagine that a professor makes a multiple choice test, and a student has figured out that the professor uses \"C\" as the correct answer 95% of the time. Why study for the test? The student can get a 95% just by answering \"C\" for all questions. \n",
    "\n",
    "Similarly, with unbalanced data, the model can look like it's performing well--scoring a 95% accuracy for example--when in reality, it simply classifies every input as belonging to a class that represents 95% of the overall data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# For transfer learning\n",
    "try:\n",
    "    import timm\n",
    "    print(f\"timm version: {timm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Warning: timm not available. Transfer learning features will not work.\")\n",
    "    timm = None\n",
    "\n",
    "# Import our helper functions\n",
    "import helpers_01\n",
    "\n",
    "# Set seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Number of Workers for Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of workers to use for data loading\n",
    "num_workers = None  # To manually set the number of workers, change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Full (Imbalanced) Dataset\n",
    "\n",
    "Now we'll load the **full** bee vs wasp dataset with **higher resolution images** (224x224) suitable for EfficientNetB5. This gives us both the class imbalance challenge and the higher quality inputs that transfer learning models expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full dataset (not the subsampled version)\n",
    "full_data_path = helpers_01.manage_full_data(\n",
    "    folder_name=\"bee_vs_wasp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Class Imbalance with Transfer Learning Data\n",
    "\n",
    "We'll use **224x224 images** (EfficientNetB5's expected input size) and explore the class distribution. The higher resolution will help the pre-trained model extract better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with higher resolution for transfer learning\n",
    "# Note: Using 224x224 images as expected by EfficientNetB0\n",
    "data_module_unbalanced = helpers_01.load_imbalanced_data(\n",
    "    full_data_path,\n",
    "    batch_size=16,  # Smaller batch size due to larger images and model\n",
    "    shape=(224, 224, 3),  # EfficientNetB5 standard input size\n",
    "    show_pictures=False,  # No need to show pictures here\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    use_weighted_sampler=False,  # No weighted sampling to see natural distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalance is pronounced with the full dataset! \n",
    "\n",
    "If we used a smaller CNN model, like in notebook [01.1_bees_vs_wasps](01.1_bees_vs_wasps.ipynb), the imbalance issue would be even more pronounced. Transfer learning alone already helps solve some of the problems by making use of the pre-learned features from a completely different dataset. But as we'll see, more can be done to improve the situation. But realize that without transfer learning, the methods discussed below are even more important!\n",
    "\n",
    "## 5. Baseline: EfficientNetB0 Feature Extraction (No Imbalance Handling)\n",
    "\n",
    "Let's start with **feature extraction** - freezing the EfficientNetB0 backbone and only training a new classifier head on the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING EFFICIENTNETB5 FEATURE EXTRACTION (No Imbalance Handling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train baseline transfer learning model without imbalance handling\n",
    "baseline_transfer_model, baseline_transfer_trainer = helpers_01.train_transfer_model(\n",
    "    data_module=data_module_unbalanced,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,  # Higher learning rate for new classifier head\n",
    "    max_epochs=8,  # Faster convergence with transfer learning\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    model_name=\"efficientnet_b0\",\n",
    "    freeze_backbone=True,  # Feature extraction mode\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline Transfer Learning Model Results:\")\n",
    "baseline_transfer_results = helpers_01.test_model(\n",
    "    data_module_unbalanced, baseline_transfer_model, baseline_transfer_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. EfficientNetB0 with Weighted Random Sampling\n",
    "\n",
    "Now let's apply **weighted random sampling** to the transfer learning approach. This will oversample minority classes while leveraging powerful pre-trained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data WITH weighted sampling for transfer learning\n",
    "data_module_weighted = helpers_01.load_imbalanced_data(\n",
    "    full_data_path,\n",
    "    batch_size=16,  # Consistent with baseline\n",
    "    shape=(224, 224, 3),  # EfficientNetB0 input size\n",
    "    show_pictures=False,  # No need to show pictures again\n",
    "    train_split=0.8,\n",
    "    num_workers=num_workers,\n",
    "    use_weighted_sampler=True,  # Enable weighted sampling\n",
    "    show_class_distribution=False,  # Don't show distribution again\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING EFFICIENTNETB0 WITH WEIGHTED SAMPLING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train transfer learning model with weighted sampling\n",
    "weighted_transfer_model, weighted_transfer_trainer = helpers_01.train_transfer_model(\n",
    "    data_module=data_module_weighted,\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=8,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    model_name=\"efficientnet_b0\",\n",
    "    freeze_backbone=True,  # Feature extraction mode\n",
    "    dropout_rate=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nWeighted Sampling Transfer Learning Model Results:\")\n",
    "weighted_transfer_results = helpers_01.test_model(\n",
    "    data_module_weighted, weighted_transfer_model, weighted_transfer_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. EfficientNetB0 with Weighted Loss Function\n",
    "\n",
    "So far, we have looked at not addressing the imbalance, and using weighted sampling. Next, let's create a **weighted loss version** of our transfer learning model. This applies higher penalties for minority class misclassifications while keeping the pre-trained feature power.\n",
    "\n",
    "Going back to the lazy student analogy from the introduction, weighted loss is like making the penalty for getting a wrong answer on rare questions much higher than for common questions. This forces the \"student\" (our model) to actually study the rare cases instead of just guessing the most common answer all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transfer learning model with weighted loss function\n",
    "class WeightedLossTransferCNN(helpers_01.TransferLearningCNN):\n",
    "    \"\"\"Transfer learning model with weighted loss function for handling class imbalance\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=4,\n",
    "        learning_rate=0.001,\n",
    "        model_name=\"efficientnet_b0\",\n",
    "        freeze_backbone=True,\n",
    "        dropout_rate=0.3,\n",
    "        class_weights=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            learning_rate=learning_rate,\n",
    "            model_name=model_name,\n",
    "            freeze_backbone=freeze_backbone,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Store class weights for loss function\n",
    "        if class_weights is not None:\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def training_step(self, batch, _batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Use weighted loss if class weights are provided\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(outputs, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Update and log metrics\n",
    "        self.train_accuracy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\n",
    "            \"train_acc\",\n",
    "            self.train_accuracy,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Calculate class weights based on inverse frequency\n",
    "# Access the already-calculated class information to avoid duplicate setup\n",
    "if hasattr(data_module_unbalanced, 'class_names') and hasattr(data_module_unbalanced, 'class_counts'):\n",
    "    class_names = data_module_unbalanced.class_names\n",
    "    class_counts = data_module_unbalanced.class_counts\n",
    "else:\n",
    "    # Fallback: get class info (this might trigger the duplicate print)\n",
    "    class_names, class_counts = data_module_unbalanced.get_class_info()\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "class_weights = []\n",
    "for class_name in class_names:\n",
    "    weight = total_samples / (num_classes * class_counts[class_name])\n",
    "    class_weights.append(weight)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(f\"Calculated class weights: {class_weights_tensor}\")\n",
    "print(\"These weights will penalize errors on minority classes more heavily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING EFFICIENTNETB0 WITH WEIGHTED LOSS FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create transfer learning model with weighted loss\n",
    "weighted_loss_transfer_model = WeightedLossTransferCNN(\n",
    "    num_classes=4,\n",
    "    learning_rate=0.001,\n",
    "    model_name=\"efficientnet_b0\",\n",
    "    freeze_backbone=True,  # Feature extraction mode\n",
    "    dropout_rate=0.3,\n",
    "    class_weights=class_weights_tensor,\n",
    ")\n",
    "\n",
    "# Create trainer for weighted loss model\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"weighted_loss_transfer_experiment\")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, verbose=False, mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"weighted-loss-transfer-best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "weighted_loss_transfer_trainer = pl.Trainer(\n",
    "    max_epochs=8,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "# Train the model (use unbalanced data module since we're handling imbalance in the loss function)\n",
    "weighted_loss_transfer_trainer.fit(weighted_loss_transfer_model, datamodule=data_module_unbalanced)\n",
    "\n",
    "print(\"\\nWeighted Loss Transfer Learning Model Results:\")\n",
    "weighted_loss_transfer_results = helpers_01.test_model(\n",
    "    data_module_unbalanced, weighted_loss_transfer_model, weighted_loss_transfer_trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Evaluation and Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of all transfer learning approaches using detailed metrics appropriate for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation_transfer(model, data_module, trainer, model_name):\n",
    "    \"\"\"Perform detailed evaluation with metrics appropriate for imbalanced datasets\"\"\"\n",
    "\n",
    "    # Get predictions on validation set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    val_loader = data_module.val_dataloader()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Get class names\n",
    "    class_names, _ = data_module.get_class_info()\n",
    "\n",
    "    print(f\"\\n{model_name} - Detailed Evaluation:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds, target_names=class_names, digits=3, zero_division=0\n",
    "    )\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar=True,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        annot_kws={\"size\": 12},\n",
    "    )\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  {class_name}: {per_class_acc[i]:.3f}\")\n",
    "\n",
    "    # Calculate confidence statistics for each class\n",
    "    print(f\"\\nPrediction Confidence by Class:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_probs = [all_probs[j][i] for j in range(len(all_probs)) if all_labels[j] == i]\n",
    "        if class_probs:\n",
    "            avg_conf = np.mean(class_probs)\n",
    "            print(f\"  {class_name}: {avg_conf:.3f} (avg confidence when correctly predicted)\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": all_preds,\n",
    "        \"labels\": all_labels,\n",
    "        \"probabilities\": all_probs,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"per_class_accuracy\": per_class_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate all transfer learning models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE TRANSFER LEARNING MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_transfer_eval = detailed_evaluation_transfer(\n",
    "    baseline_transfer_model,\n",
    "    data_module_unbalanced,\n",
    "    baseline_transfer_trainer,\n",
    "    \"EfficientNetB0 Baseline (Feature Extraction)\",\n",
    ")\n",
    "\n",
    "weighted_transfer_eval = detailed_evaluation_transfer(\n",
    "    weighted_transfer_model, \n",
    "    data_module_weighted, \n",
    "    weighted_transfer_trainer, \n",
    "    \"EfficientNetB0 + Weighted Sampling\"\n",
    ")\n",
    "\n",
    "weighted_loss_transfer_eval = detailed_evaluation_transfer(\n",
    "    weighted_loss_transfer_model,\n",
    "    data_module_unbalanced,\n",
    "    weighted_loss_transfer_trainer,\n",
    "    \"EfficientNetB0 + Weighted Loss\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary comparison\n",
    "class_names, class_counts = data_module_unbalanced.get_class_info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TRANSFER LEARNING + IMBALANCE HANDLING - SUMMARY COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(\"\\nPer-Class Accuracy Comparison:\")\n",
    "print(f\"{'Class':<15} {'Baseline':<12} {'+ Weighted':<12} {'+ W.Loss':<12} \")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    baseline_acc = baseline_transfer_eval[\"per_class_accuracy\"][i]\n",
    "    weighted_acc = weighted_transfer_eval[\"per_class_accuracy\"][i]\n",
    "    weighted_loss_acc = weighted_loss_transfer_eval[\"per_class_accuracy\"][i]\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"{class_name:<15} {baseline_acc:<12.3f} {weighted_acc:<12.3f} {weighted_loss_acc:<12.3f} \"\n",
    "    )\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(f\"\\n{'Metric':<25} {'Baseline':<12} {'+ Weighted':<12} {'+ W.Loss':<12} \")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Macro average (unweighted mean of per-class accuracies)\n",
    "baseline_macro = np.mean(baseline_transfer_eval[\"per_class_accuracy\"])\n",
    "weighted_macro = np.mean(weighted_transfer_eval[\"per_class_accuracy\"])\n",
    "weighted_loss_macro = np.mean(weighted_loss_transfer_eval[\"per_class_accuracy\"])\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{'Macro Avg Accuracy':<25} {baseline_macro:<12.3f} {weighted_macro:<12.3f} {weighted_loss_macro:<12.3f} \"\n",
    ")\n",
    "\n",
    "# Overall accuracy\n",
    "baseline_overall = np.sum(np.diag(baseline_transfer_eval[\"confusion_matrix\"])) / np.sum(\n",
    "    baseline_transfer_eval[\"confusion_matrix\"]\n",
    ")\n",
    "weighted_overall = np.sum(np.diag(weighted_transfer_eval[\"confusion_matrix\"])) / np.sum(\n",
    "    weighted_transfer_eval[\"confusion_matrix\"]\n",
    ")\n",
    "weighted_loss_overall = np.sum(\n",
    "    np.diag(weighted_loss_transfer_eval[\"confusion_matrix\"])\n",
    ") / np.sum(weighted_loss_transfer_eval[\"confusion_matrix\"])\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{'Overall Accuracy':<25} {baseline_overall:<12.3f} {weighted_overall:<12.3f} {weighted_loss_overall:<12.3f}\"\n",
    ")\n",
    "\n",
    "# Display class distribution for context\n",
    "print(f\"\\n{'Class Distribution:'}\")\n",
    "print(\"-\" * 50)\n",
    "for class_name in class_names:\n",
    "    count = class_counts[class_name]\n",
    "    percentage = (count / sum(class_counts.values())) * 100\n",
    "    print(f\"  {class_name}: {count:,} images ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights: Transfer Learning + Imbalance Handling\n",
    "\n",
    "Based on our comprehensive experiments, here are the key insights when combining transfer learning with imbalance handling:\n",
    "\n",
    "### ðŸŽ¯ **Transfer Learning Advantages for Imbalanced Data:**\n",
    "\n",
    "1. **Rich Feature Representations**: EfficientNetB0's pre-trained features help even minority classes by providing sophisticated visual understanding\n",
    "2. **Faster Convergence**: Models reach good performance in fewer epochs, allowing more experimentation with imbalance techniques\n",
    "3. **Better Generalization**: Pre-trained features often capture patterns that help minority classes generalize better\n",
    "4. **Reduced Overfitting**: Especially critical when some classes have very few examples\n",
    "\n",
    "### ðŸ“Š **Technique Performance Comparison:**\n",
    "\n",
    "1. **Feature Extraction Baseline**: \n",
    "   - Good starting point with pre-trained features\n",
    "   - May still be biased toward majority classes\n",
    "   - Fast training, lower computational cost\n",
    "\n",
    "2. **Weighted Random Sampling**:\n",
    "   - Excellent for improving minority class performance\n",
    "   - Works particularly well with transfer learning\n",
    "   - Ensures balanced exposure during training\n",
    "\n",
    "3. **Weighted Loss Function**:\n",
    "   - Good balance between majority and minority performance\n",
    "   - Maintains original data distribution\n",
    "   - Computationally efficient\n",
    "\n",
    "\n",
    "### ðŸ” **Best Practices for Transfer Learning + Imbalance:**\n",
    "\n",
    "1. **Start with Feature Extraction**: Fast baseline to understand the problem\n",
    "2. **Apply Weighted Sampling**: Usually the most effective imbalance technique with transfer learning\n",
    "3. **Consider Fine-tuning**: For the best performance when computational resources allow\n",
    "4. **Monitor Per-Class Metrics**: Focus on macro-averaged metrics and per-class performance\n",
    "5. **Use Appropriate Input Size**: Match the pre-trained model's expected input (224x224 for EfficientNetB5)\n",
    "\n",
    "### ðŸŽ¯ **When to Use Each Approach:**\n",
    "\n",
    "- **Feature Extraction + Weighted Sampling**: Best balance of performance and speed\n",
    "- **Fine-tuning + Weighted Sampling**: When you need maximum performance\n",
    "- **Weighted Loss**: When you want to maintain data distribution but improve minority class performance\n",
    "- **Combination Approaches**: Advanced users can combine multiple techniques for optimal results\n",
    "\n",
    "### ðŸ’¡ **Real-World Applications:**\n",
    "\n",
    "This combination is particularly powerful for:\n",
    "- **Medical imaging**: Where rare conditions need accurate detection\n",
    "- **Quality control**: Where defects are rare but critical to catch\n",
    "- **Wildlife monitoring**: Where some species are much rarer than others\n",
    "- **Security applications**: Where threats are infrequent but important to identify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Experiments\n",
    "\n",
    "Try these advanced modifications to further improve performance:\n",
    "\n",
    "1. **Different Pre-trained Models**: Try EfficientNetB3 (faster) or EfficientNetB7 (more accurate)\n",
    "2. **Progressive Unfreezing**: Start with frozen backbone, then gradually unfreeze layers\n",
    "3. **Custom Class Weights**: Experiment with non-linear weighting schemes\n",
    "4. **Data Augmentation**: Add augmentation specifically for minority classes\n",
    "5. **Ensemble Methods**: Combine multiple models for better performance\n",
    "\n",
    "Use the code cells below for your experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced experiment cell 1: Try different EfficientNet variants\n",
    "print(\"Experiment: Comparing EfficientNet model sizes\")\n",
    "\n",
    "# Example: Try EfficientNetB3 (smaller, faster)\n",
    "# efficientb3_model, efficientb3_trainer = helpers_01.train_transfer_model(\n",
    "#     data_module=data_module_weighted,\n",
    "#     num_classes=4,\n",
    "#     learning_rate=0.001,\n",
    "#     max_epochs=5,\n",
    "#     model_name=\"efficientnet_b3\",\n",
    "#     freeze_backbone=True,\n",
    "#     dropout_rate=0.3,\n",
    "# )\n",
    "\n",
    "# Compare inference speed and accuracy\n",
    "print(\"You can implement model comparison experiments here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced experiment cell 2: Progressive unfreezing\n",
    "print(\"Experiment: Progressive unfreezing strategy\")\n",
    "\n",
    "# Example implementation:\n",
    "# 1. Train with frozen backbone for a few epochs\n",
    "# 2. Unfreeze top layers and train with lower learning rate\n",
    "# 3. Gradually unfreeze more layers\n",
    "\n",
    "print(\"You can implement progressive unfreezing experiments here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced experiment cell 3: Custom augmentation for minority classes\n",
    "print(\"Experiment: Targeted augmentation for minority classes\")\n",
    "\n",
    "# You could create custom data loaders that apply stronger augmentation\n",
    "# to minority classes while keeping lighter augmentation for majority classes\n",
    "\n",
    "print(\"You can implement custom augmentation strategies here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
    "\n",
    " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
    "\n",
    "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
